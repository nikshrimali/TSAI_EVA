{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch",
   "display_name": "pytorch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Importing Yolo and Midas Layers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\n",
      "Now adding layers\n",
      "Model Summary: 60 layers, 1.5916e+07 parameters, 1.5916e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "from yolo_model import *\n",
    "yolo_model = get_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\n",
      "d:\\Python Projects\\EVA\\15_TheCapStone\\models_all\n",
      "Loading weights:  D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\n",
      "Using cache found in C:\\Users\\BlueFlames/.cache\\torch\\hub\\facebookresearch_WSL-Images_master\n",
      "Path D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\n"
     ]
    }
   ],
   "source": [
    "from midas_model import *\n",
    "midas_model = get_midas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ensemble import *\n",
    "model = MyEnsemble(midas_model, yolo_model).to(\"cuda\")"
   ]
  },
  {
   "source": [
    "# Summary Ensemble Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 208, 208]           9,408\n       BatchNorm2d-2         [-1, 64, 208, 208]             128\n              ReLU-3         [-1, 64, 208, 208]               0\n         MaxPool2d-4         [-1, 64, 104, 104]               0\n            Conv2d-5        [-1, 256, 104, 104]          16,384\n       BatchNorm2d-6        [-1, 256, 104, 104]             512\n              ReLU-7        [-1, 256, 104, 104]               0\n            Conv2d-8        [-1, 256, 104, 104]          18,432\n       BatchNorm2d-9        [-1, 256, 104, 104]             512\n             ReLU-10        [-1, 256, 104, 104]               0\n           Conv2d-11        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-12        [-1, 256, 104, 104]             512\n           Conv2d-13        [-1, 256, 104, 104]          16,384\n      BatchNorm2d-14        [-1, 256, 104, 104]             512\n             ReLU-15        [-1, 256, 104, 104]               0\n       Bottleneck-16        [-1, 256, 104, 104]               0\n           Conv2d-17        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-18        [-1, 256, 104, 104]             512\n             ReLU-19        [-1, 256, 104, 104]               0\n           Conv2d-20        [-1, 256, 104, 104]          18,432\n      BatchNorm2d-21        [-1, 256, 104, 104]             512\n             ReLU-22        [-1, 256, 104, 104]               0\n           Conv2d-23        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-24        [-1, 256, 104, 104]             512\n             ReLU-25        [-1, 256, 104, 104]               0\n       Bottleneck-26        [-1, 256, 104, 104]               0\n           Conv2d-27        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-28        [-1, 256, 104, 104]             512\n             ReLU-29        [-1, 256, 104, 104]               0\n           Conv2d-30        [-1, 256, 104, 104]          18,432\n      BatchNorm2d-31        [-1, 256, 104, 104]             512\n             ReLU-32        [-1, 256, 104, 104]               0\n           Conv2d-33        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-34        [-1, 256, 104, 104]             512\n             ReLU-35        [-1, 256, 104, 104]               0\n       Bottleneck-36        [-1, 256, 104, 104]               0\n           Conv2d-37        [-1, 512, 104, 104]         131,072\n      BatchNorm2d-38        [-1, 512, 104, 104]           1,024\n             ReLU-39        [-1, 512, 104, 104]               0\n           Conv2d-40          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-41          [-1, 512, 52, 52]           1,024\n             ReLU-42          [-1, 512, 52, 52]               0\n           Conv2d-43          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-44          [-1, 512, 52, 52]           1,024\n           Conv2d-45          [-1, 512, 52, 52]         131,072\n      BatchNorm2d-46          [-1, 512, 52, 52]           1,024\n             ReLU-47          [-1, 512, 52, 52]               0\n       Bottleneck-48          [-1, 512, 52, 52]               0\n           Conv2d-49          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-50          [-1, 512, 52, 52]           1,024\n             ReLU-51          [-1, 512, 52, 52]               0\n           Conv2d-52          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-53          [-1, 512, 52, 52]           1,024\n             ReLU-54          [-1, 512, 52, 52]               0\n           Conv2d-55          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-56          [-1, 512, 52, 52]           1,024\n             ReLU-57          [-1, 512, 52, 52]               0\n       Bottleneck-58          [-1, 512, 52, 52]               0\n           Conv2d-59          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-60          [-1, 512, 52, 52]           1,024\n             ReLU-61          [-1, 512, 52, 52]               0\n           Conv2d-62          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-63          [-1, 512, 52, 52]           1,024\n             ReLU-64          [-1, 512, 52, 52]               0\n           Conv2d-65          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-66          [-1, 512, 52, 52]           1,024\n             ReLU-67          [-1, 512, 52, 52]               0\n       Bottleneck-68          [-1, 512, 52, 52]               0\n           Conv2d-69          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-70          [-1, 512, 52, 52]           1,024\n             ReLU-71          [-1, 512, 52, 52]               0\n           Conv2d-72          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-73          [-1, 512, 52, 52]           1,024\n             ReLU-74          [-1, 512, 52, 52]               0\n           Conv2d-75          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-76          [-1, 512, 52, 52]           1,024\n             ReLU-77          [-1, 512, 52, 52]               0\n       Bottleneck-78          [-1, 512, 52, 52]               0\n           Conv2d-79         [-1, 1024, 52, 52]         524,288\n      BatchNorm2d-80         [-1, 1024, 52, 52]           2,048\n             ReLU-81         [-1, 1024, 52, 52]               0\n           Conv2d-82         [-1, 1024, 26, 26]         294,912\n      BatchNorm2d-83         [-1, 1024, 26, 26]           2,048\n             ReLU-84         [-1, 1024, 26, 26]               0\n           Conv2d-85         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-86         [-1, 1024, 26, 26]           2,048\n           Conv2d-87         [-1, 1024, 26, 26]         524,288\n      BatchNorm2d-88         [-1, 1024, 26, 26]           2,048\n             ReLU-89         [-1, 1024, 26, 26]               0\n       Bottleneck-90         [-1, 1024, 26, 26]               0\n           Conv2d-91         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-92         [-1, 1024, 26, 26]           2,048\n             ReLU-93         [-1, 1024, 26, 26]               0\n           Conv2d-94         [-1, 1024, 26, 26]         294,912\n      BatchNorm2d-95         [-1, 1024, 26, 26]           2,048\n             ReLU-96         [-1, 1024, 26, 26]               0\n           Conv2d-97         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-98         [-1, 1024, 26, 26]           2,048\n             ReLU-99         [-1, 1024, 26, 26]               0\n      Bottleneck-100         [-1, 1024, 26, 26]               0\n          Conv2d-101         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-102         [-1, 1024, 26, 26]           2,048\n            ReLU-103         [-1, 1024, 26, 26]               0\n          Conv2d-104         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-105         [-1, 1024, 26, 26]           2,048\n            ReLU-106         [-1, 1024, 26, 26]               0\n          Conv2d-107         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-108         [-1, 1024, 26, 26]           2,048\n            ReLU-109         [-1, 1024, 26, 26]               0\n      Bottleneck-110         [-1, 1024, 26, 26]               0\n          Conv2d-111         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-112         [-1, 1024, 26, 26]           2,048\n            ReLU-113         [-1, 1024, 26, 26]               0\n          Conv2d-114         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-115         [-1, 1024, 26, 26]           2,048\n            ReLU-116         [-1, 1024, 26, 26]               0\n          Conv2d-117         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-118         [-1, 1024, 26, 26]           2,048\n            ReLU-119         [-1, 1024, 26, 26]               0\n      Bottleneck-120         [-1, 1024, 26, 26]               0\n          Conv2d-121         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-122         [-1, 1024, 26, 26]           2,048\n            ReLU-123         [-1, 1024, 26, 26]               0\n          Conv2d-124         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-125         [-1, 1024, 26, 26]           2,048\n            ReLU-126         [-1, 1024, 26, 26]               0\n          Conv2d-127         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-128         [-1, 1024, 26, 26]           2,048\n            ReLU-129         [-1, 1024, 26, 26]               0\n      Bottleneck-130         [-1, 1024, 26, 26]               0\n          Conv2d-131         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-132         [-1, 1024, 26, 26]           2,048\n            ReLU-133         [-1, 1024, 26, 26]               0\n          Conv2d-134         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-135         [-1, 1024, 26, 26]           2,048\n            ReLU-136         [-1, 1024, 26, 26]               0\n          Conv2d-137         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-138         [-1, 1024, 26, 26]           2,048\n            ReLU-139         [-1, 1024, 26, 26]               0\n      Bottleneck-140         [-1, 1024, 26, 26]               0\n          Conv2d-141         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-142         [-1, 1024, 26, 26]           2,048\n            ReLU-143         [-1, 1024, 26, 26]               0\n          Conv2d-144         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-145         [-1, 1024, 26, 26]           2,048\n            ReLU-146         [-1, 1024, 26, 26]               0\n          Conv2d-147         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-148         [-1, 1024, 26, 26]           2,048\n            ReLU-149         [-1, 1024, 26, 26]               0\n      Bottleneck-150         [-1, 1024, 26, 26]               0\n          Conv2d-151         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-152         [-1, 1024, 26, 26]           2,048\n            ReLU-153         [-1, 1024, 26, 26]               0\n          Conv2d-154         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-155         [-1, 1024, 26, 26]           2,048\n            ReLU-156         [-1, 1024, 26, 26]               0\n          Conv2d-157         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-158         [-1, 1024, 26, 26]           2,048\n            ReLU-159         [-1, 1024, 26, 26]               0\n      Bottleneck-160         [-1, 1024, 26, 26]               0\n          Conv2d-161         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-162         [-1, 1024, 26, 26]           2,048\n            ReLU-163         [-1, 1024, 26, 26]               0\n          Conv2d-164         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-165         [-1, 1024, 26, 26]           2,048\n            ReLU-166         [-1, 1024, 26, 26]               0\n          Conv2d-167         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-168         [-1, 1024, 26, 26]           2,048\n            ReLU-169         [-1, 1024, 26, 26]               0\n      Bottleneck-170         [-1, 1024, 26, 26]               0\n          Conv2d-171         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-172         [-1, 1024, 26, 26]           2,048\n            ReLU-173         [-1, 1024, 26, 26]               0\n          Conv2d-174         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-175         [-1, 1024, 26, 26]           2,048\n            ReLU-176         [-1, 1024, 26, 26]               0\n          Conv2d-177         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-178         [-1, 1024, 26, 26]           2,048\n            ReLU-179         [-1, 1024, 26, 26]               0\n      Bottleneck-180         [-1, 1024, 26, 26]               0\n          Conv2d-181         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-182         [-1, 1024, 26, 26]           2,048\n            ReLU-183         [-1, 1024, 26, 26]               0\n          Conv2d-184         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-185         [-1, 1024, 26, 26]           2,048\n            ReLU-186         [-1, 1024, 26, 26]               0\n          Conv2d-187         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-188         [-1, 1024, 26, 26]           2,048\n            ReLU-189         [-1, 1024, 26, 26]               0\n      Bottleneck-190         [-1, 1024, 26, 26]               0\n          Conv2d-191         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-192         [-1, 1024, 26, 26]           2,048\n            ReLU-193         [-1, 1024, 26, 26]               0\n          Conv2d-194         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-195         [-1, 1024, 26, 26]           2,048\n            ReLU-196         [-1, 1024, 26, 26]               0\n          Conv2d-197         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-198         [-1, 1024, 26, 26]           2,048\n            ReLU-199         [-1, 1024, 26, 26]               0\n      Bottleneck-200         [-1, 1024, 26, 26]               0\n          Conv2d-201         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-202         [-1, 1024, 26, 26]           2,048\n            ReLU-203         [-1, 1024, 26, 26]               0\n          Conv2d-204         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-205         [-1, 1024, 26, 26]           2,048\n            ReLU-206         [-1, 1024, 26, 26]               0\n          Conv2d-207         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-208         [-1, 1024, 26, 26]           2,048\n            ReLU-209         [-1, 1024, 26, 26]               0\n      Bottleneck-210         [-1, 1024, 26, 26]               0\n          Conv2d-211         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-212         [-1, 1024, 26, 26]           2,048\n            ReLU-213         [-1, 1024, 26, 26]               0\n          Conv2d-214         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-215         [-1, 1024, 26, 26]           2,048\n            ReLU-216         [-1, 1024, 26, 26]               0\n          Conv2d-217         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-218         [-1, 1024, 26, 26]           2,048\n            ReLU-219         [-1, 1024, 26, 26]               0\n      Bottleneck-220         [-1, 1024, 26, 26]               0\n          Conv2d-221         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-222         [-1, 1024, 26, 26]           2,048\n            ReLU-223         [-1, 1024, 26, 26]               0\n          Conv2d-224         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-225         [-1, 1024, 26, 26]           2,048\n            ReLU-226         [-1, 1024, 26, 26]               0\n          Conv2d-227         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-228         [-1, 1024, 26, 26]           2,048\n            ReLU-229         [-1, 1024, 26, 26]               0\n      Bottleneck-230         [-1, 1024, 26, 26]               0\n          Conv2d-231         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-232         [-1, 1024, 26, 26]           2,048\n            ReLU-233         [-1, 1024, 26, 26]               0\n          Conv2d-234         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-235         [-1, 1024, 26, 26]           2,048\n            ReLU-236         [-1, 1024, 26, 26]               0\n          Conv2d-237         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-238         [-1, 1024, 26, 26]           2,048\n            ReLU-239         [-1, 1024, 26, 26]               0\n      Bottleneck-240         [-1, 1024, 26, 26]               0\n          Conv2d-241         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-242         [-1, 1024, 26, 26]           2,048\n            ReLU-243         [-1, 1024, 26, 26]               0\n          Conv2d-244         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-245         [-1, 1024, 26, 26]           2,048\n            ReLU-246         [-1, 1024, 26, 26]               0\n          Conv2d-247         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-248         [-1, 1024, 26, 26]           2,048\n            ReLU-249         [-1, 1024, 26, 26]               0\n      Bottleneck-250         [-1, 1024, 26, 26]               0\n          Conv2d-251         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-252         [-1, 1024, 26, 26]           2,048\n            ReLU-253         [-1, 1024, 26, 26]               0\n          Conv2d-254         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-255         [-1, 1024, 26, 26]           2,048\n            ReLU-256         [-1, 1024, 26, 26]               0\n          Conv2d-257         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-258         [-1, 1024, 26, 26]           2,048\n            ReLU-259         [-1, 1024, 26, 26]               0\n      Bottleneck-260         [-1, 1024, 26, 26]               0\n          Conv2d-261         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-262         [-1, 1024, 26, 26]           2,048\n            ReLU-263         [-1, 1024, 26, 26]               0\n          Conv2d-264         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-265         [-1, 1024, 26, 26]           2,048\n            ReLU-266         [-1, 1024, 26, 26]               0\n          Conv2d-267         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-268         [-1, 1024, 26, 26]           2,048\n            ReLU-269         [-1, 1024, 26, 26]               0\n      Bottleneck-270         [-1, 1024, 26, 26]               0\n          Conv2d-271         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-272         [-1, 1024, 26, 26]           2,048\n            ReLU-273         [-1, 1024, 26, 26]               0\n          Conv2d-274         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-275         [-1, 1024, 26, 26]           2,048\n            ReLU-276         [-1, 1024, 26, 26]               0\n          Conv2d-277         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-278         [-1, 1024, 26, 26]           2,048\n            ReLU-279         [-1, 1024, 26, 26]               0\n      Bottleneck-280         [-1, 1024, 26, 26]               0\n          Conv2d-281         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-282         [-1, 1024, 26, 26]           2,048\n            ReLU-283         [-1, 1024, 26, 26]               0\n          Conv2d-284         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-285         [-1, 1024, 26, 26]           2,048\n            ReLU-286         [-1, 1024, 26, 26]               0\n          Conv2d-287         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-288         [-1, 1024, 26, 26]           2,048\n            ReLU-289         [-1, 1024, 26, 26]               0\n      Bottleneck-290         [-1, 1024, 26, 26]               0\n          Conv2d-291         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-292         [-1, 1024, 26, 26]           2,048\n            ReLU-293         [-1, 1024, 26, 26]               0\n          Conv2d-294         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-295         [-1, 1024, 26, 26]           2,048\n            ReLU-296         [-1, 1024, 26, 26]               0\n          Conv2d-297         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-298         [-1, 1024, 26, 26]           2,048\n            ReLU-299         [-1, 1024, 26, 26]               0\n      Bottleneck-300         [-1, 1024, 26, 26]               0\n          Conv2d-301         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-302         [-1, 1024, 26, 26]           2,048\n            ReLU-303         [-1, 1024, 26, 26]               0\n          Conv2d-304         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-305         [-1, 1024, 26, 26]           2,048\n            ReLU-306         [-1, 1024, 26, 26]               0\n          Conv2d-307         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-308         [-1, 1024, 26, 26]           2,048\n            ReLU-309         [-1, 1024, 26, 26]               0\n      Bottleneck-310         [-1, 1024, 26, 26]               0\n          Conv2d-311         [-1, 2048, 26, 26]       2,097,152\n     BatchNorm2d-312         [-1, 2048, 26, 26]           4,096\n            ReLU-313         [-1, 2048, 26, 26]               0\n          Conv2d-314         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-315         [-1, 2048, 13, 13]           4,096\n            ReLU-316         [-1, 2048, 13, 13]               0\n          Conv2d-317         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-318         [-1, 2048, 13, 13]           4,096\n          Conv2d-319         [-1, 2048, 13, 13]       2,097,152\n     BatchNorm2d-320         [-1, 2048, 13, 13]           4,096\n            ReLU-321         [-1, 2048, 13, 13]               0\n      Bottleneck-322         [-1, 2048, 13, 13]               0\n          Conv2d-323         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-324         [-1, 2048, 13, 13]           4,096\n            ReLU-325         [-1, 2048, 13, 13]               0\n          Conv2d-326         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-327         [-1, 2048, 13, 13]           4,096\n            ReLU-328         [-1, 2048, 13, 13]               0\n          Conv2d-329         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-330         [-1, 2048, 13, 13]           4,096\n            ReLU-331         [-1, 2048, 13, 13]               0\n      Bottleneck-332         [-1, 2048, 13, 13]               0\n          Conv2d-333         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-334         [-1, 2048, 13, 13]           4,096\n            ReLU-335         [-1, 2048, 13, 13]               0\n          Conv2d-336         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-337         [-1, 2048, 13, 13]           4,096\n            ReLU-338         [-1, 2048, 13, 13]               0\n          Conv2d-339         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-340         [-1, 2048, 13, 13]           4,096\n            ReLU-341         [-1, 2048, 13, 13]               0\n      Bottleneck-342         [-1, 2048, 13, 13]               0\n          Conv2d-343        [-1, 256, 104, 104]         589,824\n          Conv2d-344          [-1, 256, 52, 52]       1,179,648\n          Conv2d-345          [-1, 256, 26, 26]       2,359,296\n          Conv2d-346          [-1, 256, 13, 13]       4,718,592\n            ReLU-347          [-1, 256, 13, 13]               0\n          Conv2d-348          [-1, 256, 13, 13]         590,080\n            ReLU-349          [-1, 256, 13, 13]               0\n          Conv2d-350          [-1, 256, 13, 13]         590,080\nResidualConvUnit-351          [-1, 256, 13, 13]               0\nFeatureFusionBlock-352          [-1, 256, 26, 26]               0\n            ReLU-353          [-1, 256, 26, 26]               0\n          Conv2d-354          [-1, 256, 26, 26]         590,080\n            ReLU-355          [-1, 256, 26, 26]               0\n          Conv2d-356          [-1, 256, 26, 26]         590,080\nResidualConvUnit-357          [-1, 256, 26, 26]               0\n            ReLU-358          [-1, 256, 26, 26]               0\n          Conv2d-359          [-1, 256, 26, 26]         590,080\n            ReLU-360          [-1, 256, 26, 26]               0\n          Conv2d-361          [-1, 256, 26, 26]         590,080\nResidualConvUnit-362          [-1, 256, 26, 26]               0\nFeatureFusionBlock-363          [-1, 256, 52, 52]               0\n            ReLU-364          [-1, 256, 52, 52]               0\n          Conv2d-365          [-1, 256, 52, 52]         590,080\n            ReLU-366          [-1, 256, 52, 52]               0\n          Conv2d-367          [-1, 256, 52, 52]         590,080\nResidualConvUnit-368          [-1, 256, 52, 52]               0\n            ReLU-369          [-1, 256, 52, 52]               0\n          Conv2d-370          [-1, 256, 52, 52]         590,080\n            ReLU-371          [-1, 256, 52, 52]               0\n          Conv2d-372          [-1, 256, 52, 52]         590,080\nResidualConvUnit-373          [-1, 256, 52, 52]               0\nFeatureFusionBlock-374        [-1, 256, 104, 104]               0\n            ReLU-375        [-1, 256, 104, 104]               0\n          Conv2d-376        [-1, 256, 104, 104]         590,080\n            ReLU-377        [-1, 256, 104, 104]               0\n          Conv2d-378        [-1, 256, 104, 104]         590,080\nResidualConvUnit-379        [-1, 256, 104, 104]               0\n            ReLU-380        [-1, 256, 104, 104]               0\n          Conv2d-381        [-1, 256, 104, 104]         590,080\n            ReLU-382        [-1, 256, 104, 104]               0\n          Conv2d-383        [-1, 256, 104, 104]         590,080\nResidualConvUnit-384        [-1, 256, 104, 104]               0\nFeatureFusionBlock-385        [-1, 256, 208, 208]               0\n          Conv2d-386        [-1, 128, 208, 208]         295,040\n     Interpolate-387        [-1, 128, 416, 416]               0\n          Conv2d-388         [-1, 32, 416, 416]          36,896\n            ReLU-389         [-1, 32, 416, 416]               0\n          Conv2d-390          [-1, 1, 416, 416]              33\n            ReLU-391          [-1, 1, 416, 416]               0\n          Conv2d-392         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-393         [-1, 2048, 13, 13]           4,096\n            ReLU-394         [-1, 2048, 13, 13]               0\n          Conv2d-395         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-396         [-1, 2048, 13, 13]           4,096\n            ReLU-397         [-1, 2048, 13, 13]               0\n          Conv2d-398         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-399         [-1, 2048, 13, 13]           4,096\n            ReLU-400         [-1, 2048, 13, 13]               0\n          Conv2d-401          [-1, 512, 13, 13]       1,048,576\n     BatchNorm2d-402          [-1, 512, 13, 13]           1,024\n       LeakyReLU-403          [-1, 512, 13, 13]               0\n          Conv2d-404         [-1, 1024, 13, 13]       4,718,592\n     BatchNorm2d-405         [-1, 1024, 13, 13]           2,048\n       LeakyReLU-406         [-1, 1024, 13, 13]               0\n          Conv2d-407          [-1, 512, 13, 13]         524,288\n     BatchNorm2d-408          [-1, 512, 13, 13]           1,024\n       LeakyReLU-409          [-1, 512, 13, 13]               0\n          Conv2d-410         [-1, 1024, 13, 13]       4,718,592\n     BatchNorm2d-411         [-1, 1024, 13, 13]           2,048\n       LeakyReLU-412         [-1, 1024, 13, 13]               0\n          Conv2d-413           [-1, 27, 13, 13]          27,675\n       YOLOLayer-414         [-1, 3, 13, 13, 9]               0\n          Conv2d-415          [-1, 256, 13, 13]           6,912\n     BatchNorm2d-416          [-1, 256, 13, 13]             512\n       LeakyReLU-417          [-1, 256, 13, 13]               0\n        Upsample-418          [-1, 256, 26, 26]               0\n          Conv2d-419          [-1, 256, 26, 26]          65,536\n     BatchNorm2d-420          [-1, 256, 26, 26]             512\n       LeakyReLU-421          [-1, 256, 26, 26]               0\n          Conv2d-422          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-423          [-1, 512, 26, 26]           1,024\n       LeakyReLU-424          [-1, 512, 26, 26]               0\n          Conv2d-425          [-1, 256, 26, 26]         131,072\n     BatchNorm2d-426          [-1, 256, 26, 26]             512\n       LeakyReLU-427          [-1, 256, 26, 26]               0\n          Conv2d-428          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-429          [-1, 512, 26, 26]           1,024\n       LeakyReLU-430          [-1, 512, 26, 26]               0\n          Conv2d-431          [-1, 256, 26, 26]         131,072\n     BatchNorm2d-432          [-1, 256, 26, 26]             512\n       LeakyReLU-433          [-1, 256, 26, 26]               0\n          Conv2d-434          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-435          [-1, 512, 26, 26]           1,024\n       LeakyReLU-436          [-1, 512, 26, 26]               0\n          Conv2d-437           [-1, 27, 26, 26]          13,851\n       YOLOLayer-438         [-1, 3, 26, 26, 9]               0\n          Conv2d-439          [-1, 128, 26, 26]           3,456\n     BatchNorm2d-440          [-1, 128, 26, 26]             256\n       LeakyReLU-441          [-1, 128, 26, 26]               0\n        Upsample-442          [-1, 128, 52, 52]               0\n          Conv2d-443          [-1, 128, 52, 52]          16,384\n     BatchNorm2d-444          [-1, 128, 52, 52]             256\n       LeakyReLU-445          [-1, 128, 52, 52]               0\n          Conv2d-446          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-447          [-1, 256, 52, 52]             512\n       LeakyReLU-448          [-1, 256, 52, 52]               0\n          Conv2d-449          [-1, 128, 52, 52]          32,768\n     BatchNorm2d-450          [-1, 128, 52, 52]             256\n       LeakyReLU-451          [-1, 128, 52, 52]               0\n          Conv2d-452          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-453          [-1, 256, 52, 52]             512\n       LeakyReLU-454          [-1, 256, 52, 52]               0\n          Conv2d-455          [-1, 128, 52, 52]          32,768\n     BatchNorm2d-456          [-1, 128, 52, 52]             256\n       LeakyReLU-457          [-1, 128, 52, 52]               0\n          Conv2d-458          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-459          [-1, 256, 52, 52]             512\n       LeakyReLU-460          [-1, 256, 52, 52]               0\n          Conv2d-461           [-1, 27, 52, 52]           6,939\n       YOLOLayer-462         [-1, 3, 52, 52, 9]               0\n         Darknet-463  [[-1, 3, 13, 13, 9], [-1, 3, 26, 26, 9], [-1, 3, 52, 52, 9]]               0\n================================================================\nTotal params: 233,363,410\nTrainable params: 113,264,640\nNon-trainable params: 120,098,770\n----------------------------------------------------------------\nInput size (MB): 1.98\nForward/backward pass size (MB): 6266.78\nParams size (MB): 890.21\nEstimated Total Size (MB): 7158.97\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3,416,416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\Python Projects\\EVA\\15_TheCapStone\\ensemble.h5\"\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pretrained model exists\n",
    "import os\n",
    "if os.path.exists(PATH):\n",
    "    model.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "source": [
    "# Training the Ensemble Model for Yolo Buffer Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Caching labels (19 found, 0 missing, 0 empty, 0 duplicate, for 19 images): 100%|██████████| 19/19 [00:00<00:00, 1188.83it/s]\n",
      "Caching images (0.0GB):  37%|███▋      | 7/19 [00:00<00:00, 69.54it/s]D:\\Python Projects\\EVA\\15_TheCapStone\\custom_data\\test.txt\n",
      "Caching images (0.0GB): 100%|██████████| 19/19 [00:00<00:00, 68.09it/s]\n",
      "Caching labels (39 found, 0 missing, 6 empty, 0 duplicate, for 45 images): 100%|██████████| 45/45 [00:00<00:00, 2506.79it/s]\n",
      "Caching images (0.0GB): 100%|██████████| 45/45 [00:00<00:00, 131.61it/s]\n",
      "Using 1 dataloader workers\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "       0/4      4.7G      5.19      4.42      2.84      12.4         7       512: 100%|██████████| 5/5 [00:11<00:00,  2.33s/it]\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "       1/4      4.7G      5.09      4.43      2.87      12.4         9       512: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "       2/4      4.7G      5.23      4.44      2.81      12.5        11       512: 100%|██████████| 5/5 [00:05<00:00,  1.14s/it]\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "       3/4      4.7G      5.08      4.41      2.85      12.3        10       512: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "       4/4      4.7G      5.23      4.44      2.83      12.5         9       512: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it]\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 12/12 [00:47<00:00,  3.93s/it]\n",
      "                 all        45       215         0         0   0.00157         0\n",
      "Speed: 209.7/436.2/645.8 ms inference/NMS/total per 512x512 image at batch-size 4\n"
     ]
    }
   ],
   "source": [
    "from train_ensemble import *\n",
    "train(model)"
   ]
  }
 ]
}