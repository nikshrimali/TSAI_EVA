{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch",
   "display_name": "pytorch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Yolo Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Python Projects\\\\EVA\\\\15_TheCapStone'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('D:\\Python Projects\\EVA\\TheCapStone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights from pre-trained model\n",
    "os.chdir(\"./models_all/YoloV3/\")\n",
    "\n",
    "from models import *\n",
    "from utils.parse_config import *\n",
    "\n",
    "def get_yolo():\n",
    "\n",
    "    path = './cfg/yolov3-custom.cfg'\n",
    "    yolo_model = Darknet(path).to(\"cuda\")\n",
    "\n",
    "    sc_yolokeys = list(yolo_model.state_dict().keys())\n",
    "    model_path = r\".\\pretrained_model\\best273.pt\"\n",
    "    ptmodel = torch.load(model_path)['model']\n",
    "    pt_yolokeys = (list(ptmodel.keys())[330:])\n",
    "    # Updating the Yolo model with Pre-trained weights\n",
    "    for i, keys in enumerate(sc_yolokeys):\n",
    "        yolo_model.state_dict()[keys] = ptmodel[pt_yolokeys[i]]\n",
    "    # Freeze the layers for training\n",
    "    for param in yolo_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return yolo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now adding layers\nModel Summary: 60 layers, 1.5916e+07 parameters, 1.5916e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "yolo_model = get_yolo()"
   ]
  },
  {
   "source": [
    "# Midas Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained encoder of Midas\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.chdir(r'.\\midas')\n",
    "from midas.midas_net import *\n",
    "def get_midas():\n",
    "    model_path = r\"D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\"\n",
    "    midas_model = MidasNet(model_path, non_negative=True).to(\"cuda\")\n",
    "    # Freezing layers for updates\n",
    "    for param in midas_model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    return midas_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading weights:  D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\n",
      "Using cache found in C:\\Users\\BlueFlames/.cache\\torch\\hub\\facebookresearch_WSL-Images_master\n",
      "Path D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\n"
     ]
    }
   ],
   "source": [
    "midas_model = get_midas()"
   ]
  },
  {
   "source": [
    "# Ensemble Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emsemble model\n",
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, midas, yolo_decoder, train_yolo=False, train_rcnn=False, train_all=True):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.midas = midas\n",
    "        self.yolo_decoder = yolo_decoder\n",
    "        self.train_yolo = train_yolo\n",
    "        self.train_rcnn = train_rcnn\n",
    "        self.train_all = train_all\n",
    "        # Remove last linear layer\n",
    "        # self.modelA.fc = nn.Identity()\n",
    "        # self.modelB.fc = nn.Identity()\n",
    "        \n",
    "        # Create a 3 Layer buffer between Midas Encoder and Yolo Decoder\n",
    "        # Everything would be freezed but this\n",
    "        \n",
    "        self.yolo_buffer = nn.Sequential(\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        layer_1 = self.midas.pretrained.layer1(x.clone())\n",
    "        layer_2 = self.midas.pretrained.layer2(layer_1)\n",
    "        layer_3 = self.midas.pretrained.layer3(layer_2)\n",
    "        layer_4 = self.midas.pretrained.layer4(layer_3)\n",
    "\n",
    "        # if self.train_all:\n",
    "        layer_1_rn = self.midas.scratch.layer1_rn(layer_1)\n",
    "        layer_2_rn = self.midas.scratch.layer2_rn(layer_2)\n",
    "        layer_3_rn = self.midas.scratch.layer3_rn(layer_3)\n",
    "        layer_4_rn = self.midas.scratch.layer4_rn(layer_4)\n",
    "\n",
    "        path_4 = self.midas.scratch.refinenet4(layer_4_rn)\n",
    "        path_3 = self.midas.scratch.refinenet3(path_4, layer_3_rn)\n",
    "        path_2 = self.midas.scratch.refinenet2(path_3, layer_2_rn)\n",
    "        path_1 = self.midas.scratch.refinenet1(path_2, layer_1_rn)\n",
    "\n",
    "        out = self.midas.scratch.output_conv(path_1)\n",
    "\n",
    "        # Now adding a buffer and Yolo Layers\n",
    "        yolo_buffer = self.yolo_buffer(layer_4)\n",
    "        yolo_output = self.yolo_decoder(yolo_buffer)         \n",
    "\n",
    "        # return torch.squeeze(out, dim=1), yolo_output\n",
    "        \n",
    "        # if self.train_yolo:\n",
    "        #     yolo_buffer = self.yolo_buffer(layer_4)\n",
    "        #     yolo_output = self.yolo_decoder(yolo_buffer)         \n",
    "\n",
    "        return torch.squeeze(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "model = MyEnsemble(midas_model, yolo_model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand((1,3,416,416)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 208, 208]           9,408\n       BatchNorm2d-2         [-1, 64, 208, 208]             128\n              ReLU-3         [-1, 64, 208, 208]               0\n         MaxPool2d-4         [-1, 64, 104, 104]               0\n            Conv2d-5        [-1, 256, 104, 104]          16,384\n       BatchNorm2d-6        [-1, 256, 104, 104]             512\n              ReLU-7        [-1, 256, 104, 104]               0\n            Conv2d-8        [-1, 256, 104, 104]          18,432\n       BatchNorm2d-9        [-1, 256, 104, 104]             512\n             ReLU-10        [-1, 256, 104, 104]               0\n           Conv2d-11        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-12        [-1, 256, 104, 104]             512\n           Conv2d-13        [-1, 256, 104, 104]          16,384\n      BatchNorm2d-14        [-1, 256, 104, 104]             512\n             ReLU-15        [-1, 256, 104, 104]               0\n       Bottleneck-16        [-1, 256, 104, 104]               0\n           Conv2d-17        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-18        [-1, 256, 104, 104]             512\n             ReLU-19        [-1, 256, 104, 104]               0\n           Conv2d-20        [-1, 256, 104, 104]          18,432\n      BatchNorm2d-21        [-1, 256, 104, 104]             512\n             ReLU-22        [-1, 256, 104, 104]               0\n           Conv2d-23        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-24        [-1, 256, 104, 104]             512\n             ReLU-25        [-1, 256, 104, 104]               0\n       Bottleneck-26        [-1, 256, 104, 104]               0\n           Conv2d-27        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-28        [-1, 256, 104, 104]             512\n             ReLU-29        [-1, 256, 104, 104]               0\n           Conv2d-30        [-1, 256, 104, 104]          18,432\n      BatchNorm2d-31        [-1, 256, 104, 104]             512\n             ReLU-32        [-1, 256, 104, 104]               0\n           Conv2d-33        [-1, 256, 104, 104]          65,536\n      BatchNorm2d-34        [-1, 256, 104, 104]             512\n             ReLU-35        [-1, 256, 104, 104]               0\n       Bottleneck-36        [-1, 256, 104, 104]               0\n           Conv2d-37        [-1, 512, 104, 104]         131,072\n      BatchNorm2d-38        [-1, 512, 104, 104]           1,024\n             ReLU-39        [-1, 512, 104, 104]               0\n           Conv2d-40          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-41          [-1, 512, 52, 52]           1,024\n             ReLU-42          [-1, 512, 52, 52]               0\n           Conv2d-43          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-44          [-1, 512, 52, 52]           1,024\n           Conv2d-45          [-1, 512, 52, 52]         131,072\n      BatchNorm2d-46          [-1, 512, 52, 52]           1,024\n             ReLU-47          [-1, 512, 52, 52]               0\n       Bottleneck-48          [-1, 512, 52, 52]               0\n           Conv2d-49          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-50          [-1, 512, 52, 52]           1,024\n             ReLU-51          [-1, 512, 52, 52]               0\n           Conv2d-52          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-53          [-1, 512, 52, 52]           1,024\n             ReLU-54          [-1, 512, 52, 52]               0\n           Conv2d-55          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-56          [-1, 512, 52, 52]           1,024\n             ReLU-57          [-1, 512, 52, 52]               0\n       Bottleneck-58          [-1, 512, 52, 52]               0\n           Conv2d-59          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-60          [-1, 512, 52, 52]           1,024\n             ReLU-61          [-1, 512, 52, 52]               0\n           Conv2d-62          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-63          [-1, 512, 52, 52]           1,024\n             ReLU-64          [-1, 512, 52, 52]               0\n           Conv2d-65          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-66          [-1, 512, 52, 52]           1,024\n             ReLU-67          [-1, 512, 52, 52]               0\n       Bottleneck-68          [-1, 512, 52, 52]               0\n           Conv2d-69          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-70          [-1, 512, 52, 52]           1,024\n             ReLU-71          [-1, 512, 52, 52]               0\n           Conv2d-72          [-1, 512, 52, 52]          73,728\n      BatchNorm2d-73          [-1, 512, 52, 52]           1,024\n             ReLU-74          [-1, 512, 52, 52]               0\n           Conv2d-75          [-1, 512, 52, 52]         262,144\n      BatchNorm2d-76          [-1, 512, 52, 52]           1,024\n             ReLU-77          [-1, 512, 52, 52]               0\n       Bottleneck-78          [-1, 512, 52, 52]               0\n           Conv2d-79         [-1, 1024, 52, 52]         524,288\n      BatchNorm2d-80         [-1, 1024, 52, 52]           2,048\n             ReLU-81         [-1, 1024, 52, 52]               0\n           Conv2d-82         [-1, 1024, 26, 26]         294,912\n      BatchNorm2d-83         [-1, 1024, 26, 26]           2,048\n             ReLU-84         [-1, 1024, 26, 26]               0\n           Conv2d-85         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-86         [-1, 1024, 26, 26]           2,048\n           Conv2d-87         [-1, 1024, 26, 26]         524,288\n      BatchNorm2d-88         [-1, 1024, 26, 26]           2,048\n             ReLU-89         [-1, 1024, 26, 26]               0\n       Bottleneck-90         [-1, 1024, 26, 26]               0\n           Conv2d-91         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-92         [-1, 1024, 26, 26]           2,048\n             ReLU-93         [-1, 1024, 26, 26]               0\n           Conv2d-94         [-1, 1024, 26, 26]         294,912\n      BatchNorm2d-95         [-1, 1024, 26, 26]           2,048\n             ReLU-96         [-1, 1024, 26, 26]               0\n           Conv2d-97         [-1, 1024, 26, 26]       1,048,576\n      BatchNorm2d-98         [-1, 1024, 26, 26]           2,048\n             ReLU-99         [-1, 1024, 26, 26]               0\n      Bottleneck-100         [-1, 1024, 26, 26]               0\n          Conv2d-101         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-102         [-1, 1024, 26, 26]           2,048\n            ReLU-103         [-1, 1024, 26, 26]               0\n          Conv2d-104         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-105         [-1, 1024, 26, 26]           2,048\n            ReLU-106         [-1, 1024, 26, 26]               0\n          Conv2d-107         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-108         [-1, 1024, 26, 26]           2,048\n            ReLU-109         [-1, 1024, 26, 26]               0\n      Bottleneck-110         [-1, 1024, 26, 26]               0\n          Conv2d-111         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-112         [-1, 1024, 26, 26]           2,048\n            ReLU-113         [-1, 1024, 26, 26]               0\n          Conv2d-114         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-115         [-1, 1024, 26, 26]           2,048\n            ReLU-116         [-1, 1024, 26, 26]               0\n          Conv2d-117         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-118         [-1, 1024, 26, 26]           2,048\n            ReLU-119         [-1, 1024, 26, 26]               0\n      Bottleneck-120         [-1, 1024, 26, 26]               0\n          Conv2d-121         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-122         [-1, 1024, 26, 26]           2,048\n            ReLU-123         [-1, 1024, 26, 26]               0\n          Conv2d-124         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-125         [-1, 1024, 26, 26]           2,048\n            ReLU-126         [-1, 1024, 26, 26]               0\n          Conv2d-127         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-128         [-1, 1024, 26, 26]           2,048\n            ReLU-129         [-1, 1024, 26, 26]               0\n      Bottleneck-130         [-1, 1024, 26, 26]               0\n          Conv2d-131         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-132         [-1, 1024, 26, 26]           2,048\n            ReLU-133         [-1, 1024, 26, 26]               0\n          Conv2d-134         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-135         [-1, 1024, 26, 26]           2,048\n            ReLU-136         [-1, 1024, 26, 26]               0\n          Conv2d-137         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-138         [-1, 1024, 26, 26]           2,048\n            ReLU-139         [-1, 1024, 26, 26]               0\n      Bottleneck-140         [-1, 1024, 26, 26]               0\n          Conv2d-141         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-142         [-1, 1024, 26, 26]           2,048\n            ReLU-143         [-1, 1024, 26, 26]               0\n          Conv2d-144         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-145         [-1, 1024, 26, 26]           2,048\n            ReLU-146         [-1, 1024, 26, 26]               0\n          Conv2d-147         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-148         [-1, 1024, 26, 26]           2,048\n            ReLU-149         [-1, 1024, 26, 26]               0\n      Bottleneck-150         [-1, 1024, 26, 26]               0\n          Conv2d-151         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-152         [-1, 1024, 26, 26]           2,048\n            ReLU-153         [-1, 1024, 26, 26]               0\n          Conv2d-154         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-155         [-1, 1024, 26, 26]           2,048\n            ReLU-156         [-1, 1024, 26, 26]               0\n          Conv2d-157         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-158         [-1, 1024, 26, 26]           2,048\n            ReLU-159         [-1, 1024, 26, 26]               0\n      Bottleneck-160         [-1, 1024, 26, 26]               0\n          Conv2d-161         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-162         [-1, 1024, 26, 26]           2,048\n            ReLU-163         [-1, 1024, 26, 26]               0\n          Conv2d-164         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-165         [-1, 1024, 26, 26]           2,048\n            ReLU-166         [-1, 1024, 26, 26]               0\n          Conv2d-167         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-168         [-1, 1024, 26, 26]           2,048\n            ReLU-169         [-1, 1024, 26, 26]               0\n      Bottleneck-170         [-1, 1024, 26, 26]               0\n          Conv2d-171         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-172         [-1, 1024, 26, 26]           2,048\n            ReLU-173         [-1, 1024, 26, 26]               0\n          Conv2d-174         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-175         [-1, 1024, 26, 26]           2,048\n            ReLU-176         [-1, 1024, 26, 26]               0\n          Conv2d-177         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-178         [-1, 1024, 26, 26]           2,048\n            ReLU-179         [-1, 1024, 26, 26]               0\n      Bottleneck-180         [-1, 1024, 26, 26]               0\n          Conv2d-181         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-182         [-1, 1024, 26, 26]           2,048\n            ReLU-183         [-1, 1024, 26, 26]               0\n          Conv2d-184         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-185         [-1, 1024, 26, 26]           2,048\n            ReLU-186         [-1, 1024, 26, 26]               0\n          Conv2d-187         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-188         [-1, 1024, 26, 26]           2,048\n            ReLU-189         [-1, 1024, 26, 26]               0\n      Bottleneck-190         [-1, 1024, 26, 26]               0\n          Conv2d-191         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-192         [-1, 1024, 26, 26]           2,048\n            ReLU-193         [-1, 1024, 26, 26]               0\n          Conv2d-194         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-195         [-1, 1024, 26, 26]           2,048\n            ReLU-196         [-1, 1024, 26, 26]               0\n          Conv2d-197         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-198         [-1, 1024, 26, 26]           2,048\n            ReLU-199         [-1, 1024, 26, 26]               0\n      Bottleneck-200         [-1, 1024, 26, 26]               0\n          Conv2d-201         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-202         [-1, 1024, 26, 26]           2,048\n            ReLU-203         [-1, 1024, 26, 26]               0\n          Conv2d-204         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-205         [-1, 1024, 26, 26]           2,048\n            ReLU-206         [-1, 1024, 26, 26]               0\n          Conv2d-207         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-208         [-1, 1024, 26, 26]           2,048\n            ReLU-209         [-1, 1024, 26, 26]               0\n      Bottleneck-210         [-1, 1024, 26, 26]               0\n          Conv2d-211         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-212         [-1, 1024, 26, 26]           2,048\n            ReLU-213         [-1, 1024, 26, 26]               0\n          Conv2d-214         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-215         [-1, 1024, 26, 26]           2,048\n            ReLU-216         [-1, 1024, 26, 26]               0\n          Conv2d-217         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-218         [-1, 1024, 26, 26]           2,048\n            ReLU-219         [-1, 1024, 26, 26]               0\n      Bottleneck-220         [-1, 1024, 26, 26]               0\n          Conv2d-221         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-222         [-1, 1024, 26, 26]           2,048\n            ReLU-223         [-1, 1024, 26, 26]               0\n          Conv2d-224         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-225         [-1, 1024, 26, 26]           2,048\n            ReLU-226         [-1, 1024, 26, 26]               0\n          Conv2d-227         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-228         [-1, 1024, 26, 26]           2,048\n            ReLU-229         [-1, 1024, 26, 26]               0\n      Bottleneck-230         [-1, 1024, 26, 26]               0\n          Conv2d-231         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-232         [-1, 1024, 26, 26]           2,048\n            ReLU-233         [-1, 1024, 26, 26]               0\n          Conv2d-234         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-235         [-1, 1024, 26, 26]           2,048\n            ReLU-236         [-1, 1024, 26, 26]               0\n          Conv2d-237         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-238         [-1, 1024, 26, 26]           2,048\n            ReLU-239         [-1, 1024, 26, 26]               0\n      Bottleneck-240         [-1, 1024, 26, 26]               0\n          Conv2d-241         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-242         [-1, 1024, 26, 26]           2,048\n            ReLU-243         [-1, 1024, 26, 26]               0\n          Conv2d-244         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-245         [-1, 1024, 26, 26]           2,048\n            ReLU-246         [-1, 1024, 26, 26]               0\n          Conv2d-247         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-248         [-1, 1024, 26, 26]           2,048\n            ReLU-249         [-1, 1024, 26, 26]               0\n      Bottleneck-250         [-1, 1024, 26, 26]               0\n          Conv2d-251         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-252         [-1, 1024, 26, 26]           2,048\n            ReLU-253         [-1, 1024, 26, 26]               0\n          Conv2d-254         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-255         [-1, 1024, 26, 26]           2,048\n            ReLU-256         [-1, 1024, 26, 26]               0\n          Conv2d-257         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-258         [-1, 1024, 26, 26]           2,048\n            ReLU-259         [-1, 1024, 26, 26]               0\n      Bottleneck-260         [-1, 1024, 26, 26]               0\n          Conv2d-261         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-262         [-1, 1024, 26, 26]           2,048\n            ReLU-263         [-1, 1024, 26, 26]               0\n          Conv2d-264         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-265         [-1, 1024, 26, 26]           2,048\n            ReLU-266         [-1, 1024, 26, 26]               0\n          Conv2d-267         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-268         [-1, 1024, 26, 26]           2,048\n            ReLU-269         [-1, 1024, 26, 26]               0\n      Bottleneck-270         [-1, 1024, 26, 26]               0\n          Conv2d-271         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-272         [-1, 1024, 26, 26]           2,048\n            ReLU-273         [-1, 1024, 26, 26]               0\n          Conv2d-274         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-275         [-1, 1024, 26, 26]           2,048\n            ReLU-276         [-1, 1024, 26, 26]               0\n          Conv2d-277         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-278         [-1, 1024, 26, 26]           2,048\n            ReLU-279         [-1, 1024, 26, 26]               0\n      Bottleneck-280         [-1, 1024, 26, 26]               0\n          Conv2d-281         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-282         [-1, 1024, 26, 26]           2,048\n            ReLU-283         [-1, 1024, 26, 26]               0\n          Conv2d-284         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-285         [-1, 1024, 26, 26]           2,048\n            ReLU-286         [-1, 1024, 26, 26]               0\n          Conv2d-287         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-288         [-1, 1024, 26, 26]           2,048\n            ReLU-289         [-1, 1024, 26, 26]               0\n      Bottleneck-290         [-1, 1024, 26, 26]               0\n          Conv2d-291         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-292         [-1, 1024, 26, 26]           2,048\n            ReLU-293         [-1, 1024, 26, 26]               0\n          Conv2d-294         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-295         [-1, 1024, 26, 26]           2,048\n            ReLU-296         [-1, 1024, 26, 26]               0\n          Conv2d-297         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-298         [-1, 1024, 26, 26]           2,048\n            ReLU-299         [-1, 1024, 26, 26]               0\n      Bottleneck-300         [-1, 1024, 26, 26]               0\n          Conv2d-301         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-302         [-1, 1024, 26, 26]           2,048\n            ReLU-303         [-1, 1024, 26, 26]               0\n          Conv2d-304         [-1, 1024, 26, 26]         294,912\n     BatchNorm2d-305         [-1, 1024, 26, 26]           2,048\n            ReLU-306         [-1, 1024, 26, 26]               0\n          Conv2d-307         [-1, 1024, 26, 26]       1,048,576\n     BatchNorm2d-308         [-1, 1024, 26, 26]           2,048\n            ReLU-309         [-1, 1024, 26, 26]               0\n      Bottleneck-310         [-1, 1024, 26, 26]               0\n          Conv2d-311         [-1, 2048, 26, 26]       2,097,152\n     BatchNorm2d-312         [-1, 2048, 26, 26]           4,096\n            ReLU-313         [-1, 2048, 26, 26]               0\n          Conv2d-314         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-315         [-1, 2048, 13, 13]           4,096\n            ReLU-316         [-1, 2048, 13, 13]               0\n          Conv2d-317         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-318         [-1, 2048, 13, 13]           4,096\n          Conv2d-319         [-1, 2048, 13, 13]       2,097,152\n     BatchNorm2d-320         [-1, 2048, 13, 13]           4,096\n            ReLU-321         [-1, 2048, 13, 13]               0\n      Bottleneck-322         [-1, 2048, 13, 13]               0\n          Conv2d-323         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-324         [-1, 2048, 13, 13]           4,096\n            ReLU-325         [-1, 2048, 13, 13]               0\n          Conv2d-326         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-327         [-1, 2048, 13, 13]           4,096\n            ReLU-328         [-1, 2048, 13, 13]               0\n          Conv2d-329         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-330         [-1, 2048, 13, 13]           4,096\n            ReLU-331         [-1, 2048, 13, 13]               0\n      Bottleneck-332         [-1, 2048, 13, 13]               0\n          Conv2d-333         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-334         [-1, 2048, 13, 13]           4,096\n            ReLU-335         [-1, 2048, 13, 13]               0\n          Conv2d-336         [-1, 2048, 13, 13]       1,179,648\n     BatchNorm2d-337         [-1, 2048, 13, 13]           4,096\n            ReLU-338         [-1, 2048, 13, 13]               0\n          Conv2d-339         [-1, 2048, 13, 13]       4,194,304\n     BatchNorm2d-340         [-1, 2048, 13, 13]           4,096\n            ReLU-341         [-1, 2048, 13, 13]               0\n      Bottleneck-342         [-1, 2048, 13, 13]               0\n          Conv2d-343        [-1, 256, 104, 104]         589,824\n          Conv2d-344          [-1, 256, 52, 52]       1,179,648\n          Conv2d-345          [-1, 256, 26, 26]       2,359,296\n          Conv2d-346          [-1, 256, 13, 13]       4,718,592\n            ReLU-347          [-1, 256, 13, 13]               0\n          Conv2d-348          [-1, 256, 13, 13]         590,080\n            ReLU-349          [-1, 256, 13, 13]               0\n          Conv2d-350          [-1, 256, 13, 13]         590,080\nResidualConvUnit-351          [-1, 256, 13, 13]               0\nFeatureFusionBlock-352          [-1, 256, 26, 26]               0\n            ReLU-353          [-1, 256, 26, 26]               0\n          Conv2d-354          [-1, 256, 26, 26]         590,080\n            ReLU-355          [-1, 256, 26, 26]               0\n          Conv2d-356          [-1, 256, 26, 26]         590,080\nResidualConvUnit-357          [-1, 256, 26, 26]               0\n            ReLU-358          [-1, 256, 26, 26]               0\n          Conv2d-359          [-1, 256, 26, 26]         590,080\n            ReLU-360          [-1, 256, 26, 26]               0\n          Conv2d-361          [-1, 256, 26, 26]         590,080\nResidualConvUnit-362          [-1, 256, 26, 26]               0\nFeatureFusionBlock-363          [-1, 256, 52, 52]               0\n            ReLU-364          [-1, 256, 52, 52]               0\n          Conv2d-365          [-1, 256, 52, 52]         590,080\n            ReLU-366          [-1, 256, 52, 52]               0\n          Conv2d-367          [-1, 256, 52, 52]         590,080\nResidualConvUnit-368          [-1, 256, 52, 52]               0\n            ReLU-369          [-1, 256, 52, 52]               0\n          Conv2d-370          [-1, 256, 52, 52]         590,080\n            ReLU-371          [-1, 256, 52, 52]               0\n          Conv2d-372          [-1, 256, 52, 52]         590,080\nResidualConvUnit-373          [-1, 256, 52, 52]               0\nFeatureFusionBlock-374        [-1, 256, 104, 104]               0\n            ReLU-375        [-1, 256, 104, 104]               0\n          Conv2d-376        [-1, 256, 104, 104]         590,080\n            ReLU-377        [-1, 256, 104, 104]               0\n          Conv2d-378        [-1, 256, 104, 104]         590,080\nResidualConvUnit-379        [-1, 256, 104, 104]               0\n            ReLU-380        [-1, 256, 104, 104]               0\n          Conv2d-381        [-1, 256, 104, 104]         590,080\n            ReLU-382        [-1, 256, 104, 104]               0\n          Conv2d-383        [-1, 256, 104, 104]         590,080\nResidualConvUnit-384        [-1, 256, 104, 104]               0\nFeatureFusionBlock-385        [-1, 256, 208, 208]               0\n          Conv2d-386        [-1, 128, 208, 208]         295,040\n     Interpolate-387        [-1, 128, 416, 416]               0\n          Conv2d-388         [-1, 32, 416, 416]          36,896\n            ReLU-389         [-1, 32, 416, 416]               0\n          Conv2d-390          [-1, 1, 416, 416]              33\n            ReLU-391          [-1, 1, 416, 416]               0\n          Conv2d-392         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-393         [-1, 2048, 13, 13]           4,096\n            ReLU-394         [-1, 2048, 13, 13]               0\n          Conv2d-395         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-396         [-1, 2048, 13, 13]           4,096\n            ReLU-397         [-1, 2048, 13, 13]               0\n          Conv2d-398         [-1, 2048, 13, 13]      37,750,784\n     BatchNorm2d-399         [-1, 2048, 13, 13]           4,096\n            ReLU-400         [-1, 2048, 13, 13]               0\n          Conv2d-401          [-1, 512, 13, 13]       1,048,576\n     BatchNorm2d-402          [-1, 512, 13, 13]           1,024\n       LeakyReLU-403          [-1, 512, 13, 13]               0\n          Conv2d-404         [-1, 1024, 13, 13]       4,718,592\n     BatchNorm2d-405         [-1, 1024, 13, 13]           2,048\n       LeakyReLU-406         [-1, 1024, 13, 13]               0\n          Conv2d-407          [-1, 512, 13, 13]         524,288\n     BatchNorm2d-408          [-1, 512, 13, 13]           1,024\n       LeakyReLU-409          [-1, 512, 13, 13]               0\n          Conv2d-410         [-1, 1024, 13, 13]       4,718,592\n     BatchNorm2d-411         [-1, 1024, 13, 13]           2,048\n       LeakyReLU-412         [-1, 1024, 13, 13]               0\n          Conv2d-413           [-1, 27, 13, 13]          27,675\n       YOLOLayer-414         [-1, 3, 13, 13, 9]               0\n          Conv2d-415          [-1, 256, 13, 13]           6,912\n     BatchNorm2d-416          [-1, 256, 13, 13]             512\n       LeakyReLU-417          [-1, 256, 13, 13]               0\n        Upsample-418          [-1, 256, 26, 26]               0\n          Conv2d-419          [-1, 256, 26, 26]          65,536\n     BatchNorm2d-420          [-1, 256, 26, 26]             512\n       LeakyReLU-421          [-1, 256, 26, 26]               0\n          Conv2d-422          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-423          [-1, 512, 26, 26]           1,024\n       LeakyReLU-424          [-1, 512, 26, 26]               0\n          Conv2d-425          [-1, 256, 26, 26]         131,072\n     BatchNorm2d-426          [-1, 256, 26, 26]             512\n       LeakyReLU-427          [-1, 256, 26, 26]               0\n          Conv2d-428          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-429          [-1, 512, 26, 26]           1,024\n       LeakyReLU-430          [-1, 512, 26, 26]               0\n          Conv2d-431          [-1, 256, 26, 26]         131,072\n     BatchNorm2d-432          [-1, 256, 26, 26]             512\n       LeakyReLU-433          [-1, 256, 26, 26]               0\n          Conv2d-434          [-1, 512, 26, 26]       1,179,648\n     BatchNorm2d-435          [-1, 512, 26, 26]           1,024\n       LeakyReLU-436          [-1, 512, 26, 26]               0\n          Conv2d-437           [-1, 27, 26, 26]          13,851\n       YOLOLayer-438         [-1, 3, 26, 26, 9]               0\n          Conv2d-439          [-1, 128, 26, 26]           3,456\n     BatchNorm2d-440          [-1, 128, 26, 26]             256\n       LeakyReLU-441          [-1, 128, 26, 26]               0\n        Upsample-442          [-1, 128, 52, 52]               0\n          Conv2d-443          [-1, 128, 52, 52]          16,384\n     BatchNorm2d-444          [-1, 128, 52, 52]             256\n       LeakyReLU-445          [-1, 128, 52, 52]               0\n          Conv2d-446          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-447          [-1, 256, 52, 52]             512\n       LeakyReLU-448          [-1, 256, 52, 52]               0\n          Conv2d-449          [-1, 128, 52, 52]          32,768\n     BatchNorm2d-450          [-1, 128, 52, 52]             256\n       LeakyReLU-451          [-1, 128, 52, 52]               0\n          Conv2d-452          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-453          [-1, 256, 52, 52]             512\n       LeakyReLU-454          [-1, 256, 52, 52]               0\n          Conv2d-455          [-1, 128, 52, 52]          32,768\n     BatchNorm2d-456          [-1, 128, 52, 52]             256\n       LeakyReLU-457          [-1, 128, 52, 52]               0\n          Conv2d-458          [-1, 256, 52, 52]         294,912\n     BatchNorm2d-459          [-1, 256, 52, 52]             512\n       LeakyReLU-460          [-1, 256, 52, 52]               0\n          Conv2d-461           [-1, 27, 52, 52]           6,939\n       YOLOLayer-462         [-1, 3, 52, 52, 9]               0\n         Darknet-463  [[-1, 3, 13, 13, 9], [-1, 3, 26, 26, 9], [-1, 3, 52, 52, 9]]               0\n================================================================\nTotal params: 233,363,410\nTrainable params: 113,264,640\nNon-trainable params: 120,098,770\n----------------------------------------------------------------\nInput size (MB): 1.98\nForward/backward pass size (MB): 6266.78\nParams size (MB): 890.21\nEstimated Total Size (MB): 7158.97\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3,416,416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(r\"./models_all/midas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from models_all.midas.utils import *\n",
    "import cv2\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "from midas.midas_net import MidasNet\n",
    "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "\n",
    "def run(input_path, output_path, model):\n",
    "    \"\"\"Run MonoDepthNN to compute depth maps.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): path to input folder\n",
    "        output_path (str): path to output folder\n",
    "        model_path (str): path to saved model\n",
    "    \"\"\"\n",
    "    print(\"initialize\")\n",
    "\n",
    "    # select device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device: %s\" % device)\n",
    "\n",
    "    # load network\n",
    "\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                384,\n",
    "                384,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=\"upper_bound\",\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # get input\n",
    "    img_names = glob.glob(os.path.join(input_path, \"*\"))\n",
    "    num_images = len(img_names)\n",
    "\n",
    "    # create output folder\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(\"start processing\")\n",
    "\n",
    "    for ind, img_name in enumerate(img_names):\n",
    "\n",
    "        print(\"  processing {} ({}/{})\".format(img_name, ind + 1, num_images))\n",
    "\n",
    "        # input\n",
    "\n",
    "        img = read_image(img_name)\n",
    "        img_input = transform({\"image\": img})[\"image\"]\n",
    "\n",
    "        # compute\n",
    "        with torch.no_grad():\n",
    "            sample = torch.from_numpy(img_input).to(device).unsqueeze(0)\n",
    "            prediction = model.forward(sample)\n",
    "            prediction = (\n",
    "                torch.nn.functional.interpolate(\n",
    "                    prediction.unsqueeze(1),\n",
    "                    size=img.shape[:2],\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                )\n",
    "                .squeeze()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # output\n",
    "        filename = os.path.join(\n",
    "            output_path, os.path.splitext(os.path.basename(img_name))[0]\n",
    "        )\n",
    "        write_depth(filename, prediction, bits=2)\n",
    "\n",
    "    print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initialize\n",
      "device: cuda\n",
      "start processing\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z053 .jpg (1/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z054 .jpg (2/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z055 .jpg (3/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z056 .jpg (4/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z057 .jpg (5/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z058 .jpg (6/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z059 .jpg (7/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z060 .jpg (8/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z061 .jpg (9/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z062 .jpg (10/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z063 .jpg (11/12)\n",
      "  processing D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\\z064 .jpg (12/12)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # set paths\n",
    "    INPUT_PATH = r\"D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\images\"\n",
    "    OUTPUT_PATH = r\"D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\YoloV3\\data\\customdata\\data\\output\"\n",
    "    # MODEL_PATH = \"model.pt\"\n",
    "\n",
    "    # set torch options\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # compute depth maps\n",
    "    run(INPUT_PATH, OUTPUT_PATH, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pre_trained_model = torch.load(r\"D:\\Python Projects\\EVA\\15_TheCapStone\\models_all\\midas\\model-f6b98070.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "odict_keys(['pretrained.layer1.0.weight', 'pretrained.layer1.1.weight', 'pretrained.layer1.1.bias', 'pretrained.layer1.1.running_mean', 'pretrained.layer1.1.running_var', 'pretrained.layer1.1.num_batches_tracked', 'pretrained.layer1.4.0.conv1.weight', 'pretrained.layer1.4.0.bn1.weight', 'pretrained.layer1.4.0.bn1.bias', 'pretrained.layer1.4.0.bn1.running_mean', 'pretrained.layer1.4.0.bn1.running_var', 'pretrained.layer1.4.0.bn1.num_batches_tracked', 'pretrained.layer1.4.0.conv2.weight', 'pretrained.layer1.4.0.bn2.weight', 'pretrained.layer1.4.0.bn2.bias', 'pretrained.layer1.4.0.bn2.running_mean', 'pretrained.layer1.4.0.bn2.running_var', 'pretrained.layer1.4.0.bn2.num_batches_tracked', 'pretrained.layer1.4.0.conv3.weight', 'pretrained.layer1.4.0.bn3.weight', 'pretrained.layer1.4.0.bn3.bias', 'pretrained.layer1.4.0.bn3.running_mean', 'pretrained.layer1.4.0.bn3.running_var', 'pretrained.layer1.4.0.bn3.num_batches_tracked', 'pretrained.layer1.4.0.downsample.0.weight', 'pretrained.layer1.4.0.downsample.1.weight', 'pretrained.layer1.4.0.downsample.1.bias', 'pretrained.layer1.4.0.downsample.1.running_mean', 'pretrained.layer1.4.0.downsample.1.running_var', 'pretrained.layer1.4.0.downsample.1.num_batches_tracked', 'pretrained.layer1.4.1.conv1.weight', 'pretrained.layer1.4.1.bn1.weight', 'pretrained.layer1.4.1.bn1.bias', 'pretrained.layer1.4.1.bn1.running_mean', 'pretrained.layer1.4.1.bn1.running_var', 'pretrained.layer1.4.1.bn1.num_batches_tracked', 'pretrained.layer1.4.1.conv2.weight', 'pretrained.layer1.4.1.bn2.weight', 'pretrained.layer1.4.1.bn2.bias', 'pretrained.layer1.4.1.bn2.running_mean', 'pretrained.layer1.4.1.bn2.running_var', 'pretrained.layer1.4.1.bn2.num_batches_tracked', 'pretrained.layer1.4.1.conv3.weight', 'pretrained.layer1.4.1.bn3.weight', 'pretrained.layer1.4.1.bn3.bias', 'pretrained.layer1.4.1.bn3.running_mean', 'pretrained.layer1.4.1.bn3.running_var', 'pretrained.layer1.4.1.bn3.num_batches_tracked', 'pretrained.layer1.4.2.conv1.weight', 'pretrained.layer1.4.2.bn1.weight', 'pretrained.layer1.4.2.bn1.bias', 'pretrained.layer1.4.2.bn1.running_mean', 'pretrained.layer1.4.2.bn1.running_var', 'pretrained.layer1.4.2.bn1.num_batches_tracked', 'pretrained.layer1.4.2.conv2.weight', 'pretrained.layer1.4.2.bn2.weight', 'pretrained.layer1.4.2.bn2.bias', 'pretrained.layer1.4.2.bn2.running_mean', 'pretrained.layer1.4.2.bn2.running_var', 'pretrained.layer1.4.2.bn2.num_batches_tracked', 'pretrained.layer1.4.2.conv3.weight', 'pretrained.layer1.4.2.bn3.weight', 'pretrained.layer1.4.2.bn3.bias', 'pretrained.layer1.4.2.bn3.running_mean', 'pretrained.layer1.4.2.bn3.running_var', 'pretrained.layer1.4.2.bn3.num_batches_tracked', 'pretrained.layer2.0.conv1.weight', 'pretrained.layer2.0.bn1.weight', 'pretrained.layer2.0.bn1.bias', 'pretrained.layer2.0.bn1.running_mean', 'pretrained.layer2.0.bn1.running_var', 'pretrained.layer2.0.bn1.num_batches_tracked', 'pretrained.layer2.0.conv2.weight', 'pretrained.layer2.0.bn2.weight', 'pretrained.layer2.0.bn2.bias', 'pretrained.layer2.0.bn2.running_mean', 'pretrained.layer2.0.bn2.running_var', 'pretrained.layer2.0.bn2.num_batches_tracked', 'pretrained.layer2.0.conv3.weight', 'pretrained.layer2.0.bn3.weight', 'pretrained.layer2.0.bn3.bias', 'pretrained.layer2.0.bn3.running_mean', 'pretrained.layer2.0.bn3.running_var', 'pretrained.layer2.0.bn3.num_batches_tracked', 'pretrained.layer2.0.downsample.0.weight', 'pretrained.layer2.0.downsample.1.weight', 'pretrained.layer2.0.downsample.1.bias', 'pretrained.layer2.0.downsample.1.running_mean', 'pretrained.layer2.0.downsample.1.running_var', 'pretrained.layer2.0.downsample.1.num_batches_tracked', 'pretrained.layer2.1.conv1.weight', 'pretrained.layer2.1.bn1.weight', 'pretrained.layer2.1.bn1.bias', 'pretrained.layer2.1.bn1.running_mean', 'pretrained.layer2.1.bn1.running_var', 'pretrained.layer2.1.bn1.num_batches_tracked', 'pretrained.layer2.1.conv2.weight', 'pretrained.layer2.1.bn2.weight', 'pretrained.layer2.1.bn2.bias', 'pretrained.layer2.1.bn2.running_mean', 'pretrained.layer2.1.bn2.running_var', 'pretrained.layer2.1.bn2.num_batches_tracked', 'pretrained.layer2.1.conv3.weight', 'pretrained.layer2.1.bn3.weight', 'pretrained.layer2.1.bn3.bias', 'pretrained.layer2.1.bn3.running_mean', 'pretrained.layer2.1.bn3.running_var', 'pretrained.layer2.1.bn3.num_batches_tracked', 'pretrained.layer2.2.conv1.weight', 'pretrained.layer2.2.bn1.weight', 'pretrained.layer2.2.bn1.bias', 'pretrained.layer2.2.bn1.running_mean', 'pretrained.layer2.2.bn1.running_var', 'pretrained.layer2.2.bn1.num_batches_tracked', 'pretrained.layer2.2.conv2.weight', 'pretrained.layer2.2.bn2.weight', 'pretrained.layer2.2.bn2.bias', 'pretrained.layer2.2.bn2.running_mean', 'pretrained.layer2.2.bn2.running_var', 'pretrained.layer2.2.bn2.num_batches_tracked', 'pretrained.layer2.2.conv3.weight', 'pretrained.layer2.2.bn3.weight', 'pretrained.layer2.2.bn3.bias', 'pretrained.layer2.2.bn3.running_mean', 'pretrained.layer2.2.bn3.running_var', 'pretrained.layer2.2.bn3.num_batches_tracked', 'pretrained.layer2.3.conv1.weight', 'pretrained.layer2.3.bn1.weight', 'pretrained.layer2.3.bn1.bias', 'pretrained.layer2.3.bn1.running_mean', 'pretrained.layer2.3.bn1.running_var', 'pretrained.layer2.3.bn1.num_batches_tracked', 'pretrained.layer2.3.conv2.weight', 'pretrained.layer2.3.bn2.weight', 'pretrained.layer2.3.bn2.bias', 'pretrained.layer2.3.bn2.running_mean', 'pretrained.layer2.3.bn2.running_var', 'pretrained.layer2.3.bn2.num_batches_tracked', 'pretrained.layer2.3.conv3.weight', 'pretrained.layer2.3.bn3.weight', 'pretrained.layer2.3.bn3.bias', 'pretrained.layer2.3.bn3.running_mean', 'pretrained.layer2.3.bn3.running_var', 'pretrained.layer2.3.bn3.num_batches_tracked', 'pretrained.layer3.0.conv1.weight', 'pretrained.layer3.0.bn1.weight', 'pretrained.layer3.0.bn1.bias', 'pretrained.layer3.0.bn1.running_mean', 'pretrained.layer3.0.bn1.running_var', 'pretrained.layer3.0.bn1.num_batches_tracked', 'pretrained.layer3.0.conv2.weight', 'pretrained.layer3.0.bn2.weight', 'pretrained.layer3.0.bn2.bias', 'pretrained.layer3.0.bn2.running_mean', 'pretrained.layer3.0.bn2.running_var', 'pretrained.layer3.0.bn2.num_batches_tracked', 'pretrained.layer3.0.conv3.weight', 'pretrained.layer3.0.bn3.weight', 'pretrained.layer3.0.bn3.bias', 'pretrained.layer3.0.bn3.running_mean', 'pretrained.layer3.0.bn3.running_var', 'pretrained.layer3.0.bn3.num_batches_tracked', 'pretrained.layer3.0.downsample.0.weight', 'pretrained.layer3.0.downsample.1.weight', 'pretrained.layer3.0.downsample.1.bias', 'pretrained.layer3.0.downsample.1.running_mean', 'pretrained.layer3.0.downsample.1.running_var', 'pretrained.layer3.0.downsample.1.num_batches_tracked', 'pretrained.layer3.1.conv1.weight', 'pretrained.layer3.1.bn1.weight', 'pretrained.layer3.1.bn1.bias', 'pretrained.layer3.1.bn1.running_mean', 'pretrained.layer3.1.bn1.running_var', 'pretrained.layer3.1.bn1.num_batches_tracked', 'pretrained.layer3.1.conv2.weight', 'pretrained.layer3.1.bn2.weight', 'pretrained.layer3.1.bn2.bias', 'pretrained.layer3.1.bn2.running_mean', 'pretrained.layer3.1.bn2.running_var', 'pretrained.layer3.1.bn2.num_batches_tracked', 'pretrained.layer3.1.conv3.weight', 'pretrained.layer3.1.bn3.weight', 'pretrained.layer3.1.bn3.bias', 'pretrained.layer3.1.bn3.running_mean', 'pretrained.layer3.1.bn3.running_var', 'pretrained.layer3.1.bn3.num_batches_tracked', 'pretrained.layer3.2.conv1.weight', 'pretrained.layer3.2.bn1.weight', 'pretrained.layer3.2.bn1.bias', 'pretrained.layer3.2.bn1.running_mean', 'pretrained.layer3.2.bn1.running_var', 'pretrained.layer3.2.bn1.num_batches_tracked', 'pretrained.layer3.2.conv2.weight', 'pretrained.layer3.2.bn2.weight', 'pretrained.layer3.2.bn2.bias', 'pretrained.layer3.2.bn2.running_mean', 'pretrained.layer3.2.bn2.running_var', 'pretrained.layer3.2.bn2.num_batches_tracked', 'pretrained.layer3.2.conv3.weight', 'pretrained.layer3.2.bn3.weight', 'pretrained.layer3.2.bn3.bias', 'pretrained.layer3.2.bn3.running_mean', 'pretrained.layer3.2.bn3.running_var', 'pretrained.layer3.2.bn3.num_batches_tracked', 'pretrained.layer3.3.conv1.weight', 'pretrained.layer3.3.bn1.weight', 'pretrained.layer3.3.bn1.bias', 'pretrained.layer3.3.bn1.running_mean', 'pretrained.layer3.3.bn1.running_var', 'pretrained.layer3.3.bn1.num_batches_tracked', 'pretrained.layer3.3.conv2.weight', 'pretrained.layer3.3.bn2.weight', 'pretrained.layer3.3.bn2.bias', 'pretrained.layer3.3.bn2.running_mean', 'pretrained.layer3.3.bn2.running_var', 'pretrained.layer3.3.bn2.num_batches_tracked', 'pretrained.layer3.3.conv3.weight', 'pretrained.layer3.3.bn3.weight', 'pretrained.layer3.3.bn3.bias', 'pretrained.layer3.3.bn3.running_mean', 'pretrained.layer3.3.bn3.running_var', 'pretrained.layer3.3.bn3.num_batches_tracked', 'pretrained.layer3.4.conv1.weight', 'pretrained.layer3.4.bn1.weight', 'pretrained.layer3.4.bn1.bias', 'pretrained.layer3.4.bn1.running_mean', 'pretrained.layer3.4.bn1.running_var', 'pretrained.layer3.4.bn1.num_batches_tracked', 'pretrained.layer3.4.conv2.weight', 'pretrained.layer3.4.bn2.weight', 'pretrained.layer3.4.bn2.bias', 'pretrained.layer3.4.bn2.running_mean', 'pretrained.layer3.4.bn2.running_var', 'pretrained.layer3.4.bn2.num_batches_tracked', 'pretrained.layer3.4.conv3.weight', 'pretrained.layer3.4.bn3.weight', 'pretrained.layer3.4.bn3.bias', 'pretrained.layer3.4.bn3.running_mean', 'pretrained.layer3.4.bn3.running_var', 'pretrained.layer3.4.bn3.num_batches_tracked', 'pretrained.layer3.5.conv1.weight', 'pretrained.layer3.5.bn1.weight', 'pretrained.layer3.5.bn1.bias', 'pretrained.layer3.5.bn1.running_mean', 'pretrained.layer3.5.bn1.running_var', 'pretrained.layer3.5.bn1.num_batches_tracked', 'pretrained.layer3.5.conv2.weight', 'pretrained.layer3.5.bn2.weight', 'pretrained.layer3.5.bn2.bias', 'pretrained.layer3.5.bn2.running_mean', 'pretrained.layer3.5.bn2.running_var', 'pretrained.layer3.5.bn2.num_batches_tracked', 'pretrained.layer3.5.conv3.weight', 'pretrained.layer3.5.bn3.weight', 'pretrained.layer3.5.bn3.bias', 'pretrained.layer3.5.bn3.running_mean', 'pretrained.layer3.5.bn3.running_var', 'pretrained.layer3.5.bn3.num_batches_tracked', 'pretrained.layer3.6.conv1.weight', 'pretrained.layer3.6.bn1.weight', 'pretrained.layer3.6.bn1.bias', 'pretrained.layer3.6.bn1.running_mean', 'pretrained.layer3.6.bn1.running_var', 'pretrained.layer3.6.bn1.num_batches_tracked', 'pretrained.layer3.6.conv2.weight', 'pretrained.layer3.6.bn2.weight', 'pretrained.layer3.6.bn2.bias', 'pretrained.layer3.6.bn2.running_mean', 'pretrained.layer3.6.bn2.running_var', 'pretrained.layer3.6.bn2.num_batches_tracked', 'pretrained.layer3.6.conv3.weight', 'pretrained.layer3.6.bn3.weight', 'pretrained.layer3.6.bn3.bias', 'pretrained.layer3.6.bn3.running_mean', 'pretrained.layer3.6.bn3.running_var', 'pretrained.layer3.6.bn3.num_batches_tracked', 'pretrained.layer3.7.conv1.weight', 'pretrained.layer3.7.bn1.weight', 'pretrained.layer3.7.bn1.bias', 'pretrained.layer3.7.bn1.running_mean', 'pretrained.layer3.7.bn1.running_var', 'pretrained.layer3.7.bn1.num_batches_tracked', 'pretrained.layer3.7.conv2.weight', 'pretrained.layer3.7.bn2.weight', 'pretrained.layer3.7.bn2.bias', 'pretrained.layer3.7.bn2.running_mean', 'pretrained.layer3.7.bn2.running_var', 'pretrained.layer3.7.bn2.num_batches_tracked', 'pretrained.layer3.7.conv3.weight', 'pretrained.layer3.7.bn3.weight', 'pretrained.layer3.7.bn3.bias', 'pretrained.layer3.7.bn3.running_mean', 'pretrained.layer3.7.bn3.running_var', 'pretrained.layer3.7.bn3.num_batches_tracked', 'pretrained.layer3.8.conv1.weight', 'pretrained.layer3.8.bn1.weight', 'pretrained.layer3.8.bn1.bias', 'pretrained.layer3.8.bn1.running_mean', 'pretrained.layer3.8.bn1.running_var', 'pretrained.layer3.8.bn1.num_batches_tracked', 'pretrained.layer3.8.conv2.weight', 'pretrained.layer3.8.bn2.weight', 'pretrained.layer3.8.bn2.bias', 'pretrained.layer3.8.bn2.running_mean', 'pretrained.layer3.8.bn2.running_var', 'pretrained.layer3.8.bn2.num_batches_tracked', 'pretrained.layer3.8.conv3.weight', 'pretrained.layer3.8.bn3.weight', 'pretrained.layer3.8.bn3.bias', 'pretrained.layer3.8.bn3.running_mean', 'pretrained.layer3.8.bn3.running_var', 'pretrained.layer3.8.bn3.num_batches_tracked', 'pretrained.layer3.9.conv1.weight', 'pretrained.layer3.9.bn1.weight', 'pretrained.layer3.9.bn1.bias', 'pretrained.layer3.9.bn1.running_mean', 'pretrained.layer3.9.bn1.running_var', 'pretrained.layer3.9.bn1.num_batches_tracked', 'pretrained.layer3.9.conv2.weight', 'pretrained.layer3.9.bn2.weight', 'pretrained.layer3.9.bn2.bias', 'pretrained.layer3.9.bn2.running_mean', 'pretrained.layer3.9.bn2.running_var', 'pretrained.layer3.9.bn2.num_batches_tracked', 'pretrained.layer3.9.conv3.weight', 'pretrained.layer3.9.bn3.weight', 'pretrained.layer3.9.bn3.bias', 'pretrained.layer3.9.bn3.running_mean', 'pretrained.layer3.9.bn3.running_var', 'pretrained.layer3.9.bn3.num_batches_tracked', 'pretrained.layer3.10.conv1.weight', 'pretrained.layer3.10.bn1.weight', 'pretrained.layer3.10.bn1.bias', 'pretrained.layer3.10.bn1.running_mean', 'pretrained.layer3.10.bn1.running_var', 'pretrained.layer3.10.bn1.num_batches_tracked', 'pretrained.layer3.10.conv2.weight', 'pretrained.layer3.10.bn2.weight', 'pretrained.layer3.10.bn2.bias', 'pretrained.layer3.10.bn2.running_mean', 'pretrained.layer3.10.bn2.running_var', 'pretrained.layer3.10.bn2.num_batches_tracked', 'pretrained.layer3.10.conv3.weight', 'pretrained.layer3.10.bn3.weight', 'pretrained.layer3.10.bn3.bias', 'pretrained.layer3.10.bn3.running_mean', 'pretrained.layer3.10.bn3.running_var', 'pretrained.layer3.10.bn3.num_batches_tracked', 'pretrained.layer3.11.conv1.weight', 'pretrained.layer3.11.bn1.weight', 'pretrained.layer3.11.bn1.bias', 'pretrained.layer3.11.bn1.running_mean', 'pretrained.layer3.11.bn1.running_var', 'pretrained.layer3.11.bn1.num_batches_tracked', 'pretrained.layer3.11.conv2.weight', 'pretrained.layer3.11.bn2.weight', 'pretrained.layer3.11.bn2.bias', 'pretrained.layer3.11.bn2.running_mean', 'pretrained.layer3.11.bn2.running_var', 'pretrained.layer3.11.bn2.num_batches_tracked', 'pretrained.layer3.11.conv3.weight', 'pretrained.layer3.11.bn3.weight', 'pretrained.layer3.11.bn3.bias', 'pretrained.layer3.11.bn3.running_mean', 'pretrained.layer3.11.bn3.running_var', 'pretrained.layer3.11.bn3.num_batches_tracked', 'pretrained.layer3.12.conv1.weight', 'pretrained.layer3.12.bn1.weight', 'pretrained.layer3.12.bn1.bias', 'pretrained.layer3.12.bn1.running_mean', 'pretrained.layer3.12.bn1.running_var', 'pretrained.layer3.12.bn1.num_batches_tracked', 'pretrained.layer3.12.conv2.weight', 'pretrained.layer3.12.bn2.weight', 'pretrained.layer3.12.bn2.bias', 'pretrained.layer3.12.bn2.running_mean', 'pretrained.layer3.12.bn2.running_var', 'pretrained.layer3.12.bn2.num_batches_tracked', 'pretrained.layer3.12.conv3.weight', 'pretrained.layer3.12.bn3.weight', 'pretrained.layer3.12.bn3.bias', 'pretrained.layer3.12.bn3.running_mean', 'pretrained.layer3.12.bn3.running_var', 'pretrained.layer3.12.bn3.num_batches_tracked', 'pretrained.layer3.13.conv1.weight', 'pretrained.layer3.13.bn1.weight', 'pretrained.layer3.13.bn1.bias', 'pretrained.layer3.13.bn1.running_mean', 'pretrained.layer3.13.bn1.running_var', 'pretrained.layer3.13.bn1.num_batches_tracked', 'pretrained.layer3.13.conv2.weight', 'pretrained.layer3.13.bn2.weight', 'pretrained.layer3.13.bn2.bias', 'pretrained.layer3.13.bn2.running_mean', 'pretrained.layer3.13.bn2.running_var', 'pretrained.layer3.13.bn2.num_batches_tracked', 'pretrained.layer3.13.conv3.weight', 'pretrained.layer3.13.bn3.weight', 'pretrained.layer3.13.bn3.bias', 'pretrained.layer3.13.bn3.running_mean', 'pretrained.layer3.13.bn3.running_var', 'pretrained.layer3.13.bn3.num_batches_tracked', 'pretrained.layer3.14.conv1.weight', 'pretrained.layer3.14.bn1.weight', 'pretrained.layer3.14.bn1.bias', 'pretrained.layer3.14.bn1.running_mean', 'pretrained.layer3.14.bn1.running_var', 'pretrained.layer3.14.bn1.num_batches_tracked', 'pretrained.layer3.14.conv2.weight', 'pretrained.layer3.14.bn2.weight', 'pretrained.layer3.14.bn2.bias', 'pretrained.layer3.14.bn2.running_mean', 'pretrained.layer3.14.bn2.running_var', 'pretrained.layer3.14.bn2.num_batches_tracked', 'pretrained.layer3.14.conv3.weight', 'pretrained.layer3.14.bn3.weight', 'pretrained.layer3.14.bn3.bias', 'pretrained.layer3.14.bn3.running_mean', 'pretrained.layer3.14.bn3.running_var', 'pretrained.layer3.14.bn3.num_batches_tracked', 'pretrained.layer3.15.conv1.weight', 'pretrained.layer3.15.bn1.weight', 'pretrained.layer3.15.bn1.bias', 'pretrained.layer3.15.bn1.running_mean', 'pretrained.layer3.15.bn1.running_var', 'pretrained.layer3.15.bn1.num_batches_tracked', 'pretrained.layer3.15.conv2.weight', 'pretrained.layer3.15.bn2.weight', 'pretrained.layer3.15.bn2.bias', 'pretrained.layer3.15.bn2.running_mean', 'pretrained.layer3.15.bn2.running_var', 'pretrained.layer3.15.bn2.num_batches_tracked', 'pretrained.layer3.15.conv3.weight', 'pretrained.layer3.15.bn3.weight', 'pretrained.layer3.15.bn3.bias', 'pretrained.layer3.15.bn3.running_mean', 'pretrained.layer3.15.bn3.running_var', 'pretrained.layer3.15.bn3.num_batches_tracked', 'pretrained.layer3.16.conv1.weight', 'pretrained.layer3.16.bn1.weight', 'pretrained.layer3.16.bn1.bias', 'pretrained.layer3.16.bn1.running_mean', 'pretrained.layer3.16.bn1.running_var', 'pretrained.layer3.16.bn1.num_batches_tracked', 'pretrained.layer3.16.conv2.weight', 'pretrained.layer3.16.bn2.weight', 'pretrained.layer3.16.bn2.bias', 'pretrained.layer3.16.bn2.running_mean', 'pretrained.layer3.16.bn2.running_var', 'pretrained.layer3.16.bn2.num_batches_tracked', 'pretrained.layer3.16.conv3.weight', 'pretrained.layer3.16.bn3.weight', 'pretrained.layer3.16.bn3.bias', 'pretrained.layer3.16.bn3.running_mean', 'pretrained.layer3.16.bn3.running_var', 'pretrained.layer3.16.bn3.num_batches_tracked', 'pretrained.layer3.17.conv1.weight', 'pretrained.layer3.17.bn1.weight', 'pretrained.layer3.17.bn1.bias', 'pretrained.layer3.17.bn1.running_mean', 'pretrained.layer3.17.bn1.running_var', 'pretrained.layer3.17.bn1.num_batches_tracked', 'pretrained.layer3.17.conv2.weight', 'pretrained.layer3.17.bn2.weight', 'pretrained.layer3.17.bn2.bias', 'pretrained.layer3.17.bn2.running_mean', 'pretrained.layer3.17.bn2.running_var', 'pretrained.layer3.17.bn2.num_batches_tracked', 'pretrained.layer3.17.conv3.weight', 'pretrained.layer3.17.bn3.weight', 'pretrained.layer3.17.bn3.bias', 'pretrained.layer3.17.bn3.running_mean', 'pretrained.layer3.17.bn3.running_var', 'pretrained.layer3.17.bn3.num_batches_tracked', 'pretrained.layer3.18.conv1.weight', 'pretrained.layer3.18.bn1.weight', 'pretrained.layer3.18.bn1.bias', 'pretrained.layer3.18.bn1.running_mean', 'pretrained.layer3.18.bn1.running_var', 'pretrained.layer3.18.bn1.num_batches_tracked', 'pretrained.layer3.18.conv2.weight', 'pretrained.layer3.18.bn2.weight', 'pretrained.layer3.18.bn2.bias', 'pretrained.layer3.18.bn2.running_mean', 'pretrained.layer3.18.bn2.running_var', 'pretrained.layer3.18.bn2.num_batches_tracked', 'pretrained.layer3.18.conv3.weight', 'pretrained.layer3.18.bn3.weight', 'pretrained.layer3.18.bn3.bias', 'pretrained.layer3.18.bn3.running_mean', 'pretrained.layer3.18.bn3.running_var', 'pretrained.layer3.18.bn3.num_batches_tracked', 'pretrained.layer3.19.conv1.weight', 'pretrained.layer3.19.bn1.weight', 'pretrained.layer3.19.bn1.bias', 'pretrained.layer3.19.bn1.running_mean', 'pretrained.layer3.19.bn1.running_var', 'pretrained.layer3.19.bn1.num_batches_tracked', 'pretrained.layer3.19.conv2.weight', 'pretrained.layer3.19.bn2.weight', 'pretrained.layer3.19.bn2.bias', 'pretrained.layer3.19.bn2.running_mean', 'pretrained.layer3.19.bn2.running_var', 'pretrained.layer3.19.bn2.num_batches_tracked', 'pretrained.layer3.19.conv3.weight', 'pretrained.layer3.19.bn3.weight', 'pretrained.layer3.19.bn3.bias', 'pretrained.layer3.19.bn3.running_mean', 'pretrained.layer3.19.bn3.running_var', 'pretrained.layer3.19.bn3.num_batches_tracked', 'pretrained.layer3.20.conv1.weight', 'pretrained.layer3.20.bn1.weight', 'pretrained.layer3.20.bn1.bias', 'pretrained.layer3.20.bn1.running_mean', 'pretrained.layer3.20.bn1.running_var', 'pretrained.layer3.20.bn1.num_batches_tracked', 'pretrained.layer3.20.conv2.weight', 'pretrained.layer3.20.bn2.weight', 'pretrained.layer3.20.bn2.bias', 'pretrained.layer3.20.bn2.running_mean', 'pretrained.layer3.20.bn2.running_var', 'pretrained.layer3.20.bn2.num_batches_tracked', 'pretrained.layer3.20.conv3.weight', 'pretrained.layer3.20.bn3.weight', 'pretrained.layer3.20.bn3.bias', 'pretrained.layer3.20.bn3.running_mean', 'pretrained.layer3.20.bn3.running_var', 'pretrained.layer3.20.bn3.num_batches_tracked', 'pretrained.layer3.21.conv1.weight', 'pretrained.layer3.21.bn1.weight', 'pretrained.layer3.21.bn1.bias', 'pretrained.layer3.21.bn1.running_mean', 'pretrained.layer3.21.bn1.running_var', 'pretrained.layer3.21.bn1.num_batches_tracked', 'pretrained.layer3.21.conv2.weight', 'pretrained.layer3.21.bn2.weight', 'pretrained.layer3.21.bn2.bias', 'pretrained.layer3.21.bn2.running_mean', 'pretrained.layer3.21.bn2.running_var', 'pretrained.layer3.21.bn2.num_batches_tracked', 'pretrained.layer3.21.conv3.weight', 'pretrained.layer3.21.bn3.weight', 'pretrained.layer3.21.bn3.bias', 'pretrained.layer3.21.bn3.running_mean', 'pretrained.layer3.21.bn3.running_var', 'pretrained.layer3.21.bn3.num_batches_tracked', 'pretrained.layer3.22.conv1.weight', 'pretrained.layer3.22.bn1.weight', 'pretrained.layer3.22.bn1.bias', 'pretrained.layer3.22.bn1.running_mean', 'pretrained.layer3.22.bn1.running_var', 'pretrained.layer3.22.bn1.num_batches_tracked', 'pretrained.layer3.22.conv2.weight', 'pretrained.layer3.22.bn2.weight', 'pretrained.layer3.22.bn2.bias', 'pretrained.layer3.22.bn2.running_mean', 'pretrained.layer3.22.bn2.running_var', 'pretrained.layer3.22.bn2.num_batches_tracked', 'pretrained.layer3.22.conv3.weight', 'pretrained.layer3.22.bn3.weight', 'pretrained.layer3.22.bn3.bias', 'pretrained.layer3.22.bn3.running_mean', 'pretrained.layer3.22.bn3.running_var', 'pretrained.layer3.22.bn3.num_batches_tracked', 'pretrained.layer4.0.conv1.weight', 'pretrained.layer4.0.bn1.weight', 'pretrained.layer4.0.bn1.bias', 'pretrained.layer4.0.bn1.running_mean', 'pretrained.layer4.0.bn1.running_var', 'pretrained.layer4.0.bn1.num_batches_tracked', 'pretrained.layer4.0.conv2.weight', 'pretrained.layer4.0.bn2.weight', 'pretrained.layer4.0.bn2.bias', 'pretrained.layer4.0.bn2.running_mean', 'pretrained.layer4.0.bn2.running_var', 'pretrained.layer4.0.bn2.num_batches_tracked', 'pretrained.layer4.0.conv3.weight', 'pretrained.layer4.0.bn3.weight', 'pretrained.layer4.0.bn3.bias', 'pretrained.layer4.0.bn3.running_mean', 'pretrained.layer4.0.bn3.running_var', 'pretrained.layer4.0.bn3.num_batches_tracked', 'pretrained.layer4.0.downsample.0.weight', 'pretrained.layer4.0.downsample.1.weight', 'pretrained.layer4.0.downsample.1.bias', 'pretrained.layer4.0.downsample.1.running_mean', 'pretrained.layer4.0.downsample.1.running_var', 'pretrained.layer4.0.downsample.1.num_batches_tracked', 'pretrained.layer4.1.conv1.weight', 'pretrained.layer4.1.bn1.weight', 'pretrained.layer4.1.bn1.bias', 'pretrained.layer4.1.bn1.running_mean', 'pretrained.layer4.1.bn1.running_var', 'pretrained.layer4.1.bn1.num_batches_tracked', 'pretrained.layer4.1.conv2.weight', 'pretrained.layer4.1.bn2.weight', 'pretrained.layer4.1.bn2.bias', 'pretrained.layer4.1.bn2.running_mean', 'pretrained.layer4.1.bn2.running_var', 'pretrained.layer4.1.bn2.num_batches_tracked', 'pretrained.layer4.1.conv3.weight', 'pretrained.layer4.1.bn3.weight', 'pretrained.layer4.1.bn3.bias', 'pretrained.layer4.1.bn3.running_mean', 'pretrained.layer4.1.bn3.running_var', 'pretrained.layer4.1.bn3.num_batches_tracked', 'pretrained.layer4.2.conv1.weight', 'pretrained.layer4.2.bn1.weight', 'pretrained.layer4.2.bn1.bias', 'pretrained.layer4.2.bn1.running_mean', 'pretrained.layer4.2.bn1.running_var', 'pretrained.layer4.2.bn1.num_batches_tracked', 'pretrained.layer4.2.conv2.weight', 'pretrained.layer4.2.bn2.weight', 'pretrained.layer4.2.bn2.bias', 'pretrained.layer4.2.bn2.running_mean', 'pretrained.layer4.2.bn2.running_var', 'pretrained.layer4.2.bn2.num_batches_tracked', 'pretrained.layer4.2.conv3.weight', 'pretrained.layer4.2.bn3.weight', 'pretrained.layer4.2.bn3.bias', 'pretrained.layer4.2.bn3.running_mean', 'pretrained.layer4.2.bn3.running_var', 'pretrained.layer4.2.bn3.num_batches_tracked', 'scratch.layer1_rn.weight', 'scratch.layer2_rn.weight', 'scratch.layer3_rn.weight', 'scratch.layer4_rn.weight', 'scratch.refinenet4.resConfUnit1.conv1.weight', 'scratch.refinenet4.resConfUnit1.conv1.bias', 'scratch.refinenet4.resConfUnit1.conv2.weight', 'scratch.refinenet4.resConfUnit1.conv2.bias', 'scratch.refinenet4.resConfUnit2.conv1.weight', 'scratch.refinenet4.resConfUnit2.conv1.bias', 'scratch.refinenet4.resConfUnit2.conv2.weight', 'scratch.refinenet4.resConfUnit2.conv2.bias', 'scratch.refinenet3.resConfUnit1.conv1.weight', 'scratch.refinenet3.resConfUnit1.conv1.bias', 'scratch.refinenet3.resConfUnit1.conv2.weight', 'scratch.refinenet3.resConfUnit1.conv2.bias', 'scratch.refinenet3.resConfUnit2.conv1.weight', 'scratch.refinenet3.resConfUnit2.conv1.bias', 'scratch.refinenet3.resConfUnit2.conv2.weight', 'scratch.refinenet3.resConfUnit2.conv2.bias', 'scratch.refinenet2.resConfUnit1.conv1.weight', 'scratch.refinenet2.resConfUnit1.conv1.bias', 'scratch.refinenet2.resConfUnit1.conv2.weight', 'scratch.refinenet2.resConfUnit1.conv2.bias', 'scratch.refinenet2.resConfUnit2.conv1.weight', 'scratch.refinenet2.resConfUnit2.conv1.bias', 'scratch.refinenet2.resConfUnit2.conv2.weight', 'scratch.refinenet2.resConfUnit2.conv2.bias', 'scratch.refinenet1.resConfUnit1.conv1.weight', 'scratch.refinenet1.resConfUnit1.conv1.bias', 'scratch.refinenet1.resConfUnit1.conv2.weight', 'scratch.refinenet1.resConfUnit1.conv2.bias', 'scratch.refinenet1.resConfUnit2.conv1.weight', 'scratch.refinenet1.resConfUnit2.conv1.bias', 'scratch.refinenet1.resConfUnit2.conv2.weight', 'scratch.refinenet1.resConfUnit2.conv2.bias', 'scratch.output_conv.0.weight', 'scratch.output_conv.0.bias', 'scratch.output_conv.2.weight', 'scratch.output_conv.2.bias', 'scratch.output_conv.4.weight', 'scratch.output_conv.4.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(pre_trained_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "torchsummary(pre_trained_model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('d:\\\\Python Projects\\\\EVA\\\\15_TheCapStone\\\\models_all\\\\YoloV3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from test_model import *  # import test.py to get mAP after each epoch\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "results_file = 'results.txt'\n",
    "\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 37.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.225,  # iou training threshold\n",
    "       'lr0': 0.01,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.0005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)\n",
    "\n",
    "# Overwrite hyp with hyp*.txt (optional)\n",
    "f = glob.glob('hyp*.txt')\n",
    "if f:\n",
    "    print('Using %s' % f[0])\n",
    "    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n",
    "        hyp[k] = v\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    # Value setting for hyperparameters\n",
    "    device = \"cuda\"\n",
    "    data = r'.\\data\\customdata\\custom.data' # Path in which train images are located # path = './cfg/yolov3-custom.cfg'\n",
    "    epochs = 10  # 500200 batches at bs 64, 117263 images = 273 epochs\n",
    "    batch_size = 1\n",
    "    accumulate = 4  # effective bs = batch_size * accumulate = 16 * 4 = 64\n",
    "    img_size = 512\n",
    "    imgsz_test = 512\n",
    "   \n",
    "    # Configure run\n",
    "    init_seeds() # Set seeds\n",
    "    data_dict = parse_data_cfg(data) #parse the config\n",
    "    print(data_dict['valid'])\n",
    "    train_path = r\".\\data\\customdata\\custom.txt\"\n",
    "    test_path = r\".\\data\\customdata\\custom.txt\"\n",
    "    custom_names = r\".\\data\\customdata\\custom.names\"\n",
    "    nc = 4  # number of classes\n",
    "    hyp['cls'] *= nc / 80  # update coco-tuned hyp['cls'] to current dataset\n",
    "\n",
    "    cfg = './cfg/yolov3-custom.cfg'\n",
    "\n",
    "    # Initialize model\n",
    "    # model = model\n",
    "\n",
    "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "    for k, v in dict(model.named_parameters()).items():\n",
    "        if '.bias' in k:\n",
    "            pg2 += [v]  # biases\n",
    "        elif 'Conv2d.weight' in k:\n",
    "            pg1 += [v]  # apply weight_decay\n",
    "        else:\n",
    "            pg0 += [v]  # all else\n",
    "    \n",
    "    # Initializing the optimizer\n",
    "    optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "    del pg0, pg1, pg2\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_fitness = 0.0\n",
    "\n",
    "    lf = lambda x: (((1 + math.cos(\n",
    "        x * math.pi / epochs)) / 2) ** 1.0) * 0.95 + 0.05  # cosine https://arxiv.org/pdf/1812.01187.pdf\n",
    "    \n",
    "    # train_path = \n",
    "    # LR scheduler\n",
    "#################################################################################################\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf, last_epoch=start_epoch - 1)\n",
    "\n",
    "    # Reading custom dataset and applying custom augmentation\n",
    "    dataset = LoadImagesAndLabels(train_path, img_size, batch_size,\n",
    "                                  augment=False,\n",
    "                                  hyp=hyp,  # augmentation hyperparameters\n",
    "                                  )\n",
    "############################################################################################\n",
    "    # Dataloader - Loading the dataset\n",
    "    batch_size = min(batch_size, len(dataset))\n",
    "    nw = 1 #min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=nw,\n",
    "                                             shuffle=True,  # Shuffle=True unless rectangular training isused\n",
    "                                             pin_memory=True,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "    # Testloader\n",
    "    testloader = torch.utils.data.DataLoader(LoadImagesAndLabels(test_path, imgsz_test, batch_size,hyp=hyp),\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=nw,\n",
    "                                             pin_memory=True,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "    # Model Training parameters\n",
    "    model.nc = 4  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n",
    "    # model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights konse wale\n",
    "\n",
    "    # Model EMA - Is this necessary? EMA - Moving averages weight setting\n",
    "    ema = torch_utils.ModelEMA(model)\n",
    "\n",
    "    # Start training\n",
    "    nb = len(dataloader)  # number of batches\n",
    "    n_burn = max(3 * nb, 500)  # burn-in iterations, max(3 epochs, 500 iterations)\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n",
    "    t0 = time.time()\n",
    "    # print('Image sizes %g - %g train, %g test' % (imgsz_min, imgsz_max, ))\n",
    "    \n",
    "    print('Using %g dataloader workers' % nw)\n",
    "    print('Starting training for %g epochs...' % epochs)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "        # loss\n",
    "        mloss = torch.zeros(4).to(device)  # mean losses\n",
    "        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n",
    "        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n",
    "\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Burn-in\n",
    "            if ni <= n_burn * 2:\n",
    "                model.gr = np.interp(ni, [0, n_burn * 2], [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n",
    "                if ni == n_burn:  # burnin complete\n",
    "                    print_model_biases(model)\n",
    "\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, [0, n_burn], [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, [0, n_burn], [0.9, hyp['momentum']])\n",
    "\n",
    "            # Run model\n",
    "            pred = model(imgs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss, loss_items = compute_loss(pred, targets, model)\n",
    "            if not torch.isfinite(loss):\n",
    "                print('WARNING: non-finite loss, ending training ', loss_items)\n",
    "                return results\n",
    "\n",
    "            # Scale loss by nominal batch_size of 64\n",
    "            loss *= batch_size / 64\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize accumulated gradient\n",
    "            if ni % accumulate == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                ema.update(model)\n",
    "\n",
    "            # Print batch results\n",
    "            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
    "            s = ('%10s' * 2 + '%10.3g' * 6) % ('%g/%g' % (epoch, epochs - 1), mem, *mloss, len(targets), img_size)\n",
    "            pbar.set_description(s)\n",
    "\n",
    "            # Plot images with bounding boxes\n",
    "            # print('@@@@@ ni values are',ni)\n",
    "            if ni < 1:\n",
    "                f = 'train_batch%g.png' % i  # filename\n",
    "                plot_images(imgs=imgs, targets=targets, paths=paths, fname=f)      \n",
    "\n",
    "            # end batch ------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Process epoch results\n",
    "        ema.update_attr(model)\n",
    "        final_epoch = epoch + 1 == epochs\n",
    "        # if final_epoch:  # Calculate mAP\n",
    "        is_coco = any([x in data for x in ['coco.data', 'coco2014.data', 'coco2017.data']]) and model.nc == 80\n",
    "        results, maps = test_model(cfg,\n",
    "                            data,\n",
    "                            batch_size=batch_size,\n",
    "                            img_size=imgsz_test,\n",
    "                            model=ema.ema,\n",
    "                            save_json=final_epoch and is_coco,\n",
    "                            single_cls=False,\n",
    "                            dataloader=testloader)\n",
    "\n",
    "        # Write epoch results\n",
    "        with open(results_file, 'a') as f:\n",
    "            f.write(s + '%10.3g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n",
    "               # Update best mAP\n",
    "        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n",
    "        if fi > best_fitness:\n",
    "            best_fitness = fi\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "def test_model(cfg,\n",
    "         data,\n",
    "         weights=None,\n",
    "         batch_size=16,\n",
    "         img_size=416,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,  # for nms\n",
    "         save_json=False,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         model=None,\n",
    "         dataloader=None):\n",
    "    # Initialize/load model and set device\n",
    "  \n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    verbose = False\n",
    "\n",
    "    # Configure run\n",
    "    data = parse_data_cfg(data)\n",
    "    nc = 1 if single_cls else int(data['classes'])  # number of classes\n",
    "    path = data['valid']  # path to test images\n",
    "\n",
    "\n",
    "    names = ['hardhat', 'vest', 'mask', 'boots']  # class names, yaha panga hai\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    iouv = iouv[0].view(1)  # comment for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    seen = 0\n",
    "    model.eval()\n",
    "\n",
    "    _ = model(torch.zeros((1, 3, img_size, img_size), device=device)) if device.type != 'cpu' else None  # run once\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%10s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@0.5', 'F1')\n",
    "    p, r, f1, mp, mr, map, mf1, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class = [], [], [], []\n",
    "    for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = imgs.shape  # batch size, channels, height, width\n",
    "        whwh = torch.Tensor([width, height, width, height]).to(device)\n",
    "\n",
    "        f = 'test_batch%g.png' % batch_i  # filename\n",
    "\n",
    "        plot_images(imgs=imgs, targets=targets, paths=paths, fname=f)\n",
    "\n",
    "        # Plot images with bounding boxes\n",
    "        f = 'test_batch%g.png' % batch_i  # filename\n",
    "        if batch_i < 1 and not os.path.exists(f):\n",
    "            plot_images(imgs=imgs, targets=targets, paths=paths, fname=f)\n",
    "\n",
    "        # Disable gradients\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = torch_utils.time_synchronized()\n",
    "            inf_out, train_out = model(imgs)  # inference and training outputs\n",
    "            t0 += torch_utils.time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if hasattr(model, 'hyp'):  # if model has loss hyperparameters\n",
    "                loss += compute_loss(train_out, targets, model)[1][:3]  # GIoU, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            t = torch_utils.time_synchronized()\n",
    "            output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres)  # nms\n",
    "            t1 += torch_utils.time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(output):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            seen += 1\n",
    "\n",
    "            if pred is None:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Clip boxes to image bounds\n",
    "            clip_coords(pred, (height, width))\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(Path(paths[si]).stem.split('_')[-1])\n",
    "                box = pred[:, :4].clone()  # xyxy\n",
    "                scale_coords(imgs[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n",
    "                box = xyxy2xywh(box)  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append({'image_id': image_id,\n",
    "                                  'category_id': coco91class[int(p[5])],\n",
    "                                  'bbox': [round(x, 3) for x in b],\n",
    "                                  'score': round(p[4], 5)})\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero().view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero().view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        for j in (ious > iouv[0]).nonzero():\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d not in detected:\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats):\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
    "        if niou > 1:\n",
    "            p, r, ap, f1 = p[:, 0], r[:, 0], ap.mean(1), ap[:, 0]  # [P, R, AP@0.5:0.95, AP@0.5]\n",
    "        mp, mr, map, mf1 = p.mean(), r.mean(), ap.mean(), f1.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%10.3g' * 6  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map, mf1))\n",
    "\n",
    "    # Print results per class\n",
    "    if verbose and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap[i], f1[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    if verbose or save_json:\n",
    "        t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (img_size, img_size, batch_size)  # tuple\n",
    "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map, mf1, *(loss.cpu() / len(dataloader)).tolist()), maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Caching labels (11 found, 0 missing, 0 empty, 0 duplicate, for 11 images): 100%|██████████| 11/11 [00:00<00:00, 1102.89it/s]\n",
      "Caching labels (11 found, 0 missing, 0 empty, 0 duplicate, for 11 images): 100%|██████████| 11/11 [00:00<00:00, 2221.13it/s]./customdata/data/test.txt\n",
      "Using 1 dataloader workers\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "\n",
      "       0/9     3.34G      8.71      9.44      3.16      21.3        13       512:   0%|          | 0/11 [00:02<?, ?it/s]@@@@@ ni values are 0\n",
      "       0/9     3.34G      6.81       8.4      2.76        18         6       512:  18%|█▊        | 2/11 [00:02<00:17,  1.93s/it]@@@@@ ni values are 1\n",
      "       0/9     3.34G      7.42      7.89      2.92      18.2         9       512:  27%|██▋       | 3/11 [00:03<00:11,  1.48s/it]@@@@@ ni values are 2\n",
      "       0/9     3.34G      7.61      7.72      3.03      18.4         9       512:  36%|███▋      | 4/11 [00:03<00:08,  1.16s/it]@@@@@ ni values are 3\n",
      "       0/9     3.34G      7.69      7.53      3.06      18.3         6       512:  45%|████▌     | 5/11 [00:04<00:05,  1.03it/s]@@@@@ ni values are 4\n",
      "       0/9     3.34G      7.87      7.09      3.08        18         4       512:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s]@@@@@ ni values are 5\n",
      "       0/9     3.34G      7.98      7.15      3.11      18.2         9       512:  64%|██████▎   | 7/11 [00:05<00:02,  1.47it/s]@@@@@ ni values are 6\n",
      "       0/9     3.34G      8.09      7.25      3.11      18.4         9       512:  73%|███████▎  | 8/11 [00:05<00:01,  1.65it/s]@@@@@ ni values are 7\n",
      "       0/9     3.34G      8.27       6.9      3.13      18.3         3       512:  82%|████████▏ | 9/11 [00:06<00:01,  1.70it/s]@@@@@ ni values are 8\n",
      "       0/9     3.34G      7.91       6.6      3.04      17.5         2       512:  91%|█████████ | 10/11 [00:06<00:00,  1.89it/s]@@@@@ ni values are 9\n",
      "       0/9     3.34G      7.93      6.66      3.06      17.7         8       512: 100%|██████████| 11/11 [00:06<00:00,  2.03it/s]@@@@@ ni values are 10\n",
      "       0/9     3.34G      7.93      6.66      3.06      17.7         8       512: 100%|██████████| 11/11 [00:07<00:00,  1.53it/s]\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1:   0%|          | 0/11 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'f' referenced before assignment",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9d3046b5d0d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-199ec7462830>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    210\u001b[0m                             \u001b[0msave_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_epoch\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_coco\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                             \u001b[0msingle_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                             dataloader=testloader)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;31m# Write epoch results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-3cc40cb1e064>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(cfg, data, weights, batch_size, img_size, conf_thres, iou_thres, save_json, single_cls, augment, model, dataloader)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mwhwh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mplot_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Plot images with bounding boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'f' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.yolo_decoder.module_list[1]"
   ]
  },
  {
   "source": [
    "# Planercnn model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WinError 3] The system cannot find the path specified: '.\\\\models_all\\\\planercnn'\nd:\\Python Projects\\EVA\\15_TheCapStone\n"
     ]
    }
   ],
   "source": [
    "%cd \".\\models_all\\planercnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.model'; 'models' is not a package",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5fbc2c57d82a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefinement_net\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models.model'; 'models' is not a package"
     ]
    }
   ],
   "source": [
    "# from planercnn.train_planercnn import *\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "from models.model import *\n",
    "from models.refinement_net import *\n",
    "from models.modules import *\n",
    "from datasets.plane_stereo_dataset import *\n",
    "\n",
    "from utils import *\n",
    "from visualize_utils import *\n",
    "from evaluate_utils import *\n",
    "from options import parse_args\n",
    "from config import PlaneConfig\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "def rand_func(**kwargs):\n",
    "    # print('Received kwargs', kwargs)\n",
    "    args = Namespace(LR=1e-05,\n",
    "        MaskRCNNPath='../mask_rcnn_coco.pth',\n",
    "        anchorFolder='anchors/',\n",
    "        anchorType='normal',\n",
    "        batchSize=16, blocks='', checkpoint_dir='checkpoint/planercnn_normal_warping_refine',\n",
    "        considerPartial=False, convType='2', cornerLocationNoise=0.0, cornerPositiveWeight=0,\n",
    "        correctionType='one', customDataFolder='test/custom', dataFolder='../../Data/ScanNet/',\n",
    "        dataset='', debug=False, distanceThreshold2D=20, distanceThreshold3D=0.2, frameGap=20,\n",
    "        gpu=1, heatmapThreshold=0.5, height=512, keyname='planercnn_normal_warping_refine',\n",
    "        locationNoise=0.0, losses='', maskHeight=56, maskWeight=1, maskWidth=56, methods='b',\n",
    "        minNumPointRatio=0.05, modelType='', numAnchorPlanes=0, numEpochs=1000, numInputChannels=4,\n",
    "        numNodes=10, numPositiveExamples=200, numTestingImages=100, numTrainingImages=1000,\n",
    "        numViews=0, occlusionNoise=0, outputDim=256, planeAreaThreshold=500, planeWidthThreshold=10,\n",
    "        positiveWeight=0.33, predictAdjacency=False, restore=2, savePoints=False, scaleMode='variant',\n",
    "        startEpoch=-1, suffix='warping_refine', task='train', test_dir='test/planercnn_normal_warping_refine',\n",
    "        testingDataset='', testingIndex=-1, trainingMode='all', visualizeMode='', warpingWeight=0.1, width=640\n",
    "        )\n",
    "\n",
    "    config = PlaneConfig(args)\n",
    "    model = MaskRCNN(config)\n",
    "    model.load_state_dict(torch.load('/content/planercnn/checkpoint/planercnn_normal_warping_refine/checkpoint.pth'))\n",
    "    return model\n"
   ]
  }
 ]
}